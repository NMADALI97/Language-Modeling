{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "TP_Analyse_de_sentiments_DS_telecom_20.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "C1JgjehCmd4b",
        "ERNk5kZTmd7Q",
        "yGu9qAZ5md9o"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH1UgN5lmd1o",
        "colab_type": "text"
      },
      "source": [
        "# TP : Analyse de sentiments dans les critiques de films"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgKDRO_Wmd1z",
        "colab_type": "text"
      },
      "source": [
        "## Objectifs\n",
        "\n",
        "1. Implémenter une manière simple de représenter des données textuelles - Bag of words\n",
        "2. Implémenter un modèle d'apprentissage statistique basique - Bayésien Naïf\n",
        "3. Utiliser ces représentations et ce modèle pour une tâche d'analyse de sentiments\n",
        "4. Implémenter différentes manières d'obtenir des représentations denses des mêmes données\n",
        "5. Utiliser un modèle de régression logistique pour entraîner un classifieur sur ces nouvelles représentations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IaAQ2Axu33e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b52286ff-2370-4937-c114-d859ee38b507"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1Sq1OVZzjFmxmtbKvwgRXW1I6HCNWfofx\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"data_and_images.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "    "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Sq1OVZzjFmxmtbKvwgRXW1I6HCNWfofx\n",
            "To: /content/data_and_images.zip\n",
            "23.4MB [00:00, 56.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nQT9fE0md17",
        "colab_type": "text"
      },
      "source": [
        "## Dépendances nécessaires\n",
        "\n",
        "Pour commencer, on aura besoin des packages suivants:\n",
        "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
        "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
        "\n",
        "Les deux sont disponibles avec Anaconda: https://anaconda.org/anaconda/nltk et https://anaconda.org/anaconda/scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYcUMqd5md2F",
        "colab_type": "code",
        "outputId": "b6875603-7296-4d57-e4db-804b5dad29d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os.path as op\n",
        "import re \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeD__0_Xmd2i",
        "colab_type": "text"
      },
      "source": [
        "## Charger les données\n",
        "\n",
        "On récupère les données textuelles dans la variable *texts*\n",
        "\n",
        "On récupère les labels dans la variable $y$ qui en contient *len(texts)* : $0$ indique que la critique correspondante est négative tandis que $1$ qu'elle est positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTVsFfIhmd2q",
        "colab_type": "code",
        "outputId": "4cf32503-c6d9-41da-b58a-6529469ec0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from glob import glob\n",
        "# We get the files from the path: ./data/imdb1/neg for negative reviews, and ./data/imdb1/pos for positive reviews\n",
        "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
        "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
        "\n",
        "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
        "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
        "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
        "texts = texts_neg + texts_pos\n",
        "\n",
        "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
        "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
        "y = np.ones(len(texts), dtype=np.int)\n",
        "y[:len(texts_neg)] = 0.\n",
        "\n",
        "print(\"%d documents\" % len(texts))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 documents\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urbtnfyimd3A",
        "colab_type": "code",
        "outputId": "83951248-306c-4508-9fa4-aad5e5580f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
        "# Use an even number to keep the same number of positive and negative reviews\n",
        "k = 10\n",
        "texts_reduced = texts[0::k]\n",
        "y_reduced = y [0::k]\n",
        "\n",
        "print('Nombre de documents:', len(texts_reduced))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de documents: 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ovXAwFbmd3d",
        "colab_type": "text"
      },
      "source": [
        "# Bayésien Naïf \n",
        "\n",
        "## Idée principale\n",
        "\n",
        "On dispose d'une critique étant en fait une liste de mots $s = (w_1, ..., w_N)$, et l'on cherche à trouver la classe associée $c$ - qui dans notre cas, peut-être $c = 0$ ou $c = 1$. L'objectif est donc de trouver pour chaque critique $s$ la classe $\\hat{c}$ maximisant la probabilité conditionelle **$P(c|s)$** : \n",
        "\n",
        "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
        "\n",
        "**Hypothèse : P(s) est constante pour chaque classe** :\n",
        "\n",
        "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
        "\n",
        "**Hypothèse naïve : les différentes variables (mots) d'une critique sont indépendantes entre elles** : \n",
        "\n",
        "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
        "\n",
        "On va donc pouvoir se servir des critiques annotées à notre disposition pour **estimer les probabilités $P(w|c)$ pour chaque mot $w$ étant donné les deux classes $c$**. Ces critiques vont nous permettre d'apprendre à évaluer la \"compatibilité\" entre les mots et classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxhm-W9emd3m",
        "colab_type": "text"
      },
      "source": [
        "## Vue générale\n",
        "\n",
        "### Entraînement: Estimer les probabilités\n",
        "\n",
        "Pour chaque mot $w$ du vocabulaire $V$, $P(w|c)$ est le nombre d'occurences de $w$ dans une critique ayant pour classe $c$, divisé par le nombre total d'occurences dans $c$. Si on note $T(w,c)$ ce nombre d'occurences, on obtient:\n",
        "\n",
        "$$P(w|c) = \\text{Fréquence de }w\\text{ dans }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
        "\n",
        "### Test: Calcul des scores\n",
        "\n",
        "Pour faciliter les calculs et éviter les erreurs d'*underflow* et d'approximation, on utilise le \"log-sum trick\", et on passe l'équation en log-probabilités : \n",
        "\n",
        "$$\\hat{c} =  \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\, \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right]$$\n",
        "\n",
        "### Laplace smoothing (Lissage)\n",
        "\n",
        "Un mot qui n'apparaît pas dans un document a une probabilité nulle: cela va poser problème avec le logarithme. On garde donc une toute petite partie de la masse de probabilité qu'on redistribue avec le *Laplace smoothing*: \n",
        "\n",
        "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$$\n",
        "\n",
        "Il existe d'autre méthodes de lissage, en général adaptées à d'autres applications plus complexes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKsqACyqmd3u",
        "colab_type": "text"
      },
      "source": [
        "## Représentation adaptée des documents\n",
        "\n",
        "Notre modèle statistique, comme la plupart des modèles appliqués aux données textuelles, utilise les comptes d'occurences de mots dans un document. Ainsi, une manière très pratique de représenter un document est d'utiliser un vecteur \"Bag-of-Words\" (BoW), contenant les comptes de chaque mot (indifférement de leur ordre d'apparition) dans le document. \n",
        "\n",
        "Si on considère l'ensemble de tous les mots apparaissant dans nos $T$ documents d'apprentissage, que l'on note $V$ (Vocabulaire), on peut créer **un index**, qui est une bijection associant à chaque mot $w$ un entier, qui sera sa position dans $V$. \n",
        "\n",
        "Ainsi, pour un document extrait d'un ensemble de documents contenant $|V|$ mots différents, une représentation BoW sera un vecteur de taille $|V|$, dont la valeur à l'indice d'un mot $w$ sera son nombre d'occurences dans le document. \n",
        "\n",
        "On peut utiliser la classe **CountVectorizer** de scikit-learn pour mieux comprendre:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH01PQW1md33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLyj3U9md4F",
        "colab_type": "code",
        "outputId": "a2519f83-781e-4f86-87ae-96d6129ba3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "Bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "Bow.toarray()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 2, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyf3jkj7md4X",
        "colab_type": "text"
      },
      "source": [
        "On affiche d'abord la liste contenant les mots ordonnés selon leur indice (On note que les mots de 2 caractères ou moins ne sont pas pris en compte)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1JgjehCmd4b",
        "colab_type": "text"
      },
      "source": [
        "## Détail: entraînement\n",
        "\n",
        "L'idée est d'extraire le nombre d'occurences $T(w,c)$ de chaque mot $w$ pour chaque classe $c$, ce qui permettra de calculer la matrice de probabilités conditionelles $\\pmb{P}$ telle que: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
        "\n",
        "Notons que le nombre d'occurences $T(w,c)$ peut être obtenu facilement à partir des représentations BoW de l'ensemble des documents.\n",
        "\n",
        "### Procédure:\n",
        "<img src=\"algo_train.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
        "\n",
        "## Détail: test\n",
        "\n",
        "Nous connaissons maintenant les probabilités conditionelles données par la matrice $\\pmb{P}$. \n",
        "Il faut maintenant obtenir $P(s|c)$ pour le document courant. Cette quantité s'obtient à l'aide d'un calcul simple impliquant la représentation BoW du document et $\\pmb{P}$.\n",
        "\n",
        "### Procédure:\n",
        "<img src=\"algo_test.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxZIzh5md4h",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing du texte: obtenir les représentations BoW\n",
        "\n",
        "D'abord, il faut transformer les critiques sous forme de strings en une liste de mots. La tactique la plus simple consiste à diviser le string suivant les espaces, avec la commande:\n",
        "```text.split()```\n",
        "\n",
        "Mais il faut aussi faire attention à enlever les caractères particuliers qui pourraient ne pas avoir été nettoyés (comme les balises HTML si on a obtenu les données à partir de pages web). Puisque l'on va compter les mots, il faudra construire une liste des mots apparaissant dans nos données. Dans notre cas, on aimerait réduire cette liste et l'uniformiser (ignorer les majuscules, la ponctuation, et les mots les plus courts). \n",
        "On va donc utiliser une fonction adaptée à nos besoins - mais c'est un travail qu'il n'est en général pas nécessaire de faire, puisqu'il existe de nombreux outils déjà adaptés à la plupart des cas de figures. \n",
        "Pour le nettoyage du texte, il existe de nombreux scripts, basés sur différents outils (expressions régulières, par exemple) qui permettent de préparer des données. La division du texte en mots et la gestion de la ponctuation est gérée lors d'une étape appellée *tokenization*; si besoin, un package python comme le NLTK contient de nombreux *tokenizers* différents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aNYrUf8md4o",
        "colab_type": "code",
        "outputId": "94ec8ff2-72ce-4a0f-8385-e0f3ec98ff64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# We might want to clean the file with various strategies:\n",
        "def clean_and_tokenize(text,remove_stopwords=False):\n",
        "    \"\"\"\n",
        "    Cleaning a document with:\n",
        "        - Lowercase        \n",
        "        - Removing numbers with regular expressions\n",
        "        - Removing punctuation with regular expressions\n",
        "        - Removing other artifacts\n",
        "    And separate the document into words by simply splitting at spaces\n",
        "    Params:\n",
        "        text (string): a sentence or a document\n",
        "    Returns:\n",
        "        tokens (list of strings): the list of tokens (word units) forming the document\n",
        "    \"\"\"        \n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
        "    # Remove punctuation\n",
        "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "    text = REMOVE_PUNCT.sub(\"\", text)\n",
        "    # Remove small words (1 and statistique2 characters)\n",
        "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
        "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
        "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "    text = REPLACE_HTML.sub(\" \", text)\n",
        "    \n",
        "    tokens = text.split()  \n",
        "    if  remove_stopwords :\n",
        "      tokens =[u for u in tokens if not u in  stopwords.words('english') ] \n",
        "    return tokens\n",
        "\n",
        "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
        "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
        "print(clean_and_tokenize(corpus_raw))\n",
        "print(word_tokenize(corpus_raw))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['walked', 'down', 'down', 'the', 'boulevard', 'walked', 'down', 'the', 'avenue', 'ran', 'down', 'the', 'boulevard', 'walk', 'down', 'the', 'city', 'walk', 'down', 'the', 'the', 'avenue']\n",
            "['I', 'walked', 'down', 'down', 'the', 'boulevard', '.', 'I', 'walked', 'down', 'the', 'avenue', '.', 'I', 'ran', 'down', 'the', 'boulevard', '.', 'I', 'walk', 'down', 'the', 'city', '.', 'I', 'walk', 'down', 'the', 'the', 'avenue', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLHZtHmamd44",
        "colab_type": "text"
      },
      "source": [
        "Fonction **à compléter**. Elle prend en entrée une liste de document (chacun sous la forme d'un string) et renvoie, comme dans l'exemple utilisant ```CountVectorizer```:\n",
        "- Un vocabulaire qui associe à chaque mot rencontré un index\n",
        "- Une matrice, dont les lignes représentent les documents et les colonnes les mots indexés par le vocabulaire. En position $(i,j)$, on devra avoir le nombre d'occurence du mot $j$ dans le document $i$.\n",
        "\n",
        "Le vocabulaire, qui était sous la forme d'une *liste* dans l'exemple précédent, pourra être renvoyé sous forme de *dictionnaire* dont les clés sont les mots et les valeurs les indices. Puisque le vocabulaire recense les mots du corpus sans se soucier de leur nombre d'occurences, on pourra le constituer à l'aide d'un ensemble (```set``` en python). \n",
        "On pourra bien sur utiliser la fonction ```clean_and_tokenize``` pour transformer les strings en liste de mots. \n",
        "##### Quelques pointeurs pour les débutants en Python : \n",
        "\n",
        "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
        "\n",
        "-  ```words = set()``` : create a set, which is a list of unique values \n",
        "\n",
        "- ```words.union(my_list)``` : extend the set ```words```\n",
        "\n",
        "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
        "\n",
        "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
        "\n",
        "- ```len(my-list)``` : length of the list ```my_list```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItNl3Dxdmd4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(texts,remove_stopwords=False):\n",
        "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : list of str\n",
        "        The texts\n",
        "    Returns\n",
        "    -------\n",
        "    vocabulary : dict\n",
        "        A dictionary that points to an index in counts for each word.\n",
        "    counts : ndarray, shape (n_samples, n_features)\n",
        "        The counts of each word in each text.\n",
        "    \"\"\"\n",
        "    lists=stopwords.words('english')\n",
        "    vocabulary={}\n",
        "    idx=0\n",
        "    for text in texts:\n",
        "      tokens=clean_and_tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in vocabulary.keys() :\n",
        "          if remove_stopwords :\n",
        "            if  not token in lists:\n",
        "              vocabulary[token]=idx\n",
        "              idx+=1\n",
        "          else :\n",
        "              vocabulary[token]=idx\n",
        "              idx+=1\n",
        "    sorted_vocabulary=dict(sorted(vocabulary.items()))\n",
        "    \n",
        "    vocabulary=dict((i, w) for w, i in enumerate(sorted_vocabulary.keys()))\n",
        "    counts=np.zeros((len(texts),len(vocabulary)))\n",
        "   \n",
        "    for i,text in enumerate(texts):\n",
        "      tokens=clean_and_tokenize(text)\n",
        "      for token in tokens:\n",
        "        if remove_stopwords :\n",
        "          if  not token in lists:\n",
        "            counts[i,vocabulary[token]]+=1\n",
        "        else :\n",
        "          counts[i,vocabulary[token]]+=1\n",
        "\n",
        "    return vocabulary, counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OzPfOZV_RlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arva-VSKmd5P",
        "colab_type": "code",
        "outputId": "4517c4ea-fb16-4321-99a3-7ea096432b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "Voc, X = count_words(corpus)\n",
        "print(Voc)\n",
        "print(X)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'avenue': 0, 'boulevard': 1, 'city': 2, 'down': 3, 'ran': 4, 'the': 5, 'walk': 6, 'walked': 7}\n",
            "[[0. 1. 0. 2. 0. 1. 0. 1.]\n",
            " [1. 0. 0. 1. 0. 1. 0. 1.]\n",
            " [0. 1. 0. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 1. 1. 0. 1. 1. 0.]\n",
            " [1. 0. 0. 1. 0. 2. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG03GmB8md5g",
        "colab_type": "text"
      },
      "source": [
        "## Bayésien Naïf \n",
        "\n",
        "Classe vide : fonctions **à compléter** : \n",
        "\n",
        "```python\n",
        "def fit(self, X, y)\n",
        "``` \n",
        "**Entraînement** : va apprendre un modèle statistique basés sur les représentations $X$ correspondant aux labels $y$.\n",
        "$X$ représente donc ici des représentations obtenues en sortie de count_words. On complète la fonction à l'aide de la procédure détaillée plus haut. Si il est possible de la suivre à la lettre, les représentations que l'on utilise nous permettre d'être bien plus efficace et d'éviter d'utiliser des boucles !\n",
        "\n",
        "\n",
        "Note: le lissage, effectué à la ligne $10$, ne se fait pas nécessairement avec un $1$, mais peut se faire avec une valeur positive $\\alpha$, qu'on peut implémenter comme argument de la classe ```NB```.\n",
        "\n",
        "```python\n",
        "def predict(self, X)\n",
        "```\n",
        "**Testing** : va renvoyer les labels prédits par le modèle pour d'autres représentations $X$.\n",
        "\n",
        "\n",
        "Pour faciliter la procédure, on prendra la moitié de la matrice $X$ obtenue plus haut pour entraîner le modèle, et l'autre moitié pour l'évaluer. **Important**: cette façon de procéder n'est pas réaliste: en général, on ne dispose que des données d'entraînement au moment de créer le vocabulaire et d'entraîner le modèle. Ainsi, il est possible que les données d'évaluation contiennent des mots *inconnus*. C'est quelque chose qu'on peut traiter facilement en dédiant un indice à tous les mots rencontrés qui ne sont pas contenus dans le vocabulaire - mais il existe de nombreuses méthodes plus complexes pour réussir à utiliser à bon escient ces mots que le modèle n'a pas rencontré à l'entraînement. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NR06wrDmd5k",
        "colab_type": "text"
      },
      "source": [
        "##### Quelques pointeurs pour les débutants en Python : \n",
        "\n",
        "Utiliser l'API Numpy pour travailler avec des tenseurs\n",
        "\n",
        "\n",
        "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
        "\n",
        "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
        "\n",
        "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
        "\n",
        "- ```np.mean(X, axis = n)```\n",
        "\n",
        "- ```np.argmax(X, axis = n)```\n",
        "\n",
        "- ```np.log(X)```\n",
        "\n",
        "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHUGhe-Pmd5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NB(BaseEstimator, ClassifierMixin):\n",
        "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
        "    def __init__(self, alpha=1.0):\n",
        "        # alpha est un paramètre pour le lissage: il correspond à la valeur ligne 10 de l'algorithme d'entraînement\n",
        "        # Dans l'algorithme d'entraînement, et comme valeur par défaut, on utilise alpha = 1\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.V=X.shape[1]\n",
        "       \n",
        "        idx0=[i for i,u in enumerate(y) if u==0]\n",
        "        idx1=[i for i,u in enumerate(y) if u==1]\n",
        "        self.P=np.zeros((2,self.V))\n",
        "\n",
        "        X0=X[idx0,:]\n",
        "        X1=X[idx1,:]\n",
        "\n",
        "        self.P[0]=(np.sum(X0,axis=0)+self.alpha)/(X0.sum()+ self.alpha*self.V)\n",
        "        self.P[1]=(np.sum(X1,axis=0)+self.alpha)/(X1.sum()+ self.alpha*self.V)\n",
        "\n",
        "        self.Py=np.array([len(idx0)/len(y),len(idx1)/len(y)])\n",
        "\n",
        "        \n",
        " \n",
        "    def predict(self, texts):\n",
        "        result=[]\n",
        "        for X in texts: \n",
        "          tmp=np.zeros(2)\n",
        "          for i in range(2):\n",
        "            tmp[i]=np.log(self.Py[i])\n",
        "            tmp[i]+=np.sum(X@np.log(self.P[i]))\n",
        "          result.append(np.argmax(tmp))\n",
        "        return result\n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict(X) == y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq3LGTzhmd54",
        "colab_type": "text"
      },
      "source": [
        "## Expérimentation\n",
        "\n",
        "On utilise la moitié des données pour l'entraînement, l'autre pour tester le modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlnVArOtmd59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc, X = count_words(texts_reduced,remove_stopwords=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_vr8vfhymd6I",
        "colab_type": "code",
        "outputId": "3d3aebe8-2b60-4023-e712-4113372f6c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nb = NB()\n",
        "nb.fit(X[::2], y_reduced[::2])\n",
        "print(nb.score(X[1::2], y_reduced[1::2]))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkjKK8mLmd6V",
        "colab_type": "text"
      },
      "source": [
        "## Cross-validation \n",
        "\n",
        "Avec la fonction *cross_val_score* de scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CDC2H52md6Z",
        "colab_type": "code",
        "outputId": "cb6efa4a-e479-4252-c8d3-6a97e0f3d9ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score de classification: 0.7804 (std 0.022249494376277386)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut8sGvgHmd6m",
        "colab_type": "text"
      },
      "source": [
        "## Evaluer les performances: \n",
        "\n",
        "**Quelles sont les points forts et les points faibles de ce système ? Comment y remédier ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDD48xL7SPt",
        "colab_type": "text"
      },
      "source": [
        "- Smoothing allows several words to have a non-null probability\n",
        "- Adjust the smoothing parameter to improve performance\n",
        "- Choose a prior distribution for the probability of words\n",
        "- Choose a bigram or trigram model to improve performance\n",
        "- Remove stop words which make bias  estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7cwKhHImd6q",
        "colab_type": "text"
      },
      "source": [
        "## Pour aller plus loin: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7qt0UMsmd6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzENZ4ymd68",
        "colab_type": "text"
      },
      "source": [
        "## Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9zG2Spnmd7A",
        "colab_type": "text"
      },
      "source": [
        "### Améliorer les représentations\n",
        "\n",
        "On utilise la function \n",
        "```python\n",
        "CountVectorizer\n",
        "``` \n",
        "de scikit-learn pour constituer notre corpus. Elle nous permettra d'améliorer facilement nos représentations BoW.\n",
        "\n",
        "#### Tf-idf:\n",
        "\n",
        "Il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF).\n",
        "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice d'occurences $mots \\times documents$. Ainsi, pour une matrice $\\mathbf{T}$ de $|V|$ termes et $D$ documents:\n",
        "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
        "\n",
        "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
        "\n",
        "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
        "\n",
        "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. \n",
        "    \n",
        "#### Ne pas prendre en compte les mots trop fréquents:\n",
        "\n",
        "On peut utiliser l'option \n",
        "```python\n",
        "max_df=1.0\n",
        "```\n",
        "pour modifier la quantité de mots pris en compte. \n",
        "\n",
        "#### Essayer différentes granularités:\n",
        "\n",
        "Plutôt que de simplement compter les mots, on peut compter les séquences de mots - de taille limitée, bien sur. \n",
        "On appelle une séquence de $n$ mots un $n$-gram: essayons d'utiliser les 2 et 3-grams (bi- et trigrams).\n",
        "On peut aussi tenter d'utiliser les séquences de caractères à la place de séquences de mots.\n",
        "\n",
        "On s'intéressera aux options \n",
        "```python\n",
        "analyzer='word'\n",
        "```\n",
        "et \n",
        "```python\n",
        "ngram_range=(1, 2)\n",
        "```\n",
        "que l'on changera pour modifier la granularité. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "56CrhZ6vmd7D",
        "colab_type": "code",
        "outputId": "087ab4f7-b1af-4c55-ccaf-bf9495aab5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "## On peut définir une pipeline que l'on modifiera pour expérimenter.\n",
        "\n",
        "pipeline_base = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_base, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_tf_idf = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "    ('Tf_Idf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_tf_idf, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score tf-idf: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_maxdf = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word',max_df=0.6, stop_words=None)),\n",
        "    ('Tf_Idf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_maxdf, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score sans mots fréquents: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_bigram = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word',ngram_range=(2,2), stop_words=None)),\n",
        "    ('Tf_Idf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_bigram, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score bigram: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_trigram = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word',ngram_range=(3,3), stop_words=None)),\n",
        "    ('Tf_Idf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_trigram, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score trigram: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_char = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='char', stop_words=None)),\n",
        "    ('Tf_Idf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_char, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score char: %s (std %s)\",(np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification score: %s (std %s) (0.7807999999999999, 0.0213391658693586)\n",
            "Classification score tf-idf: %s (std %s) (0.7832, 0.023583044756773856)\n",
            "Classification score sans mots fréquents: %s (std %s) (0.7876000000000001, 0.021813757127097547)\n",
            "Classification score bigram: %s (std %s) (0.8051999999999999, 0.018998947339260643)\n",
            "Classification score trigram: %s (std %s) (0.7364, 0.016560193235587575)\n",
            "Classification score char: %s (std %s) (0.6136, 0.0209914268214431)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERNk5kZTmd7Q",
        "colab_type": "text"
      },
      "source": [
        "### Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY3at3M3md7V",
        "colab_type": "text"
      },
      "source": [
        "### Stemming \n",
        "\n",
        "Permet de revenir à la racine d'un mot: on peut ainsi grouper différents mots autour de la même racine, ce qui facilite la généralisation. Utiliser:\n",
        "```python\n",
        "from nltk import SnowballStemmer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfr7QkrFmd7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzLw4AkImd7j",
        "colab_type": "text"
      },
      "source": [
        "#### Exemple d'utilisation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEJztx-ymd7m",
        "colab_type": "code",
        "outputId": "2a70f702-86a1-4712-a0d1-c4726070f9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
        "for word in words:\n",
        "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word : singers ; stemmed : singer\n",
            "word : cat ; stemmed : cat\n",
            "word : generalization ; stemmed : general\n",
            "word : philosophy ; stemmed : philosophi\n",
            "word : psychology ; stemmed : psycholog\n",
            "word : philosopher ; stemmed : philosoph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWI25iGZmd71",
        "colab_type": "text"
      },
      "source": [
        "#### Transformation des données:\n",
        "\n",
        "Classe vide : function **à compléter** \n",
        "```python\n",
        "def stem(X)\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN_SOlLLmd75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem(X): \n",
        "    X_stem = []\n",
        "    for text in X:\n",
        "      tokens=text.split(\" \")\n",
        "      tmp=\"\".join(stemmer.stem(u)+\" \" for u in  tokens)\n",
        "      X_stem.append(tmp)\n",
        "    return X_stem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHz6ONLjmd8D",
        "colab_type": "code",
        "outputId": "08adf1c7-0138-4d2b-aac0-62f947237e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "texts_stemmed = stem(texts_reduced)\n",
        "voc, X = count_words(texts_stemmed)\n",
        "nb = NB()\n",
        "\n",
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score de classification: 0.7692 (std 0.017092688495377213)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD487D0Xmd8K",
        "colab_type": "text"
      },
      "source": [
        "### Partie du discours\n",
        "\n",
        "Pour généraliser, on peut aussi utiliser les parties du discours (Part of Speech, POS) des mots, ce qui nous permettra  de filtrer l'information qui n'est potentiellement pas utile au modèle. On va récupérer les POS des mots à l'aide des fonctions:\n",
        "```python\n",
        "from nltk import pos_tag, word_tokenize\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlEYvUXMmd8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oci0ScQpmd8Y",
        "colab_type": "text"
      },
      "source": [
        "#### Exemple d'utilisation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kLbMGQkSmd8b",
        "colab_type": "code",
        "outputId": "36ac39e5-3d7c-4b18-8b63-9a9f0079541c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pos_tag(word_tokenize(('I am Sam')))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('Sam', 'NNP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ2DGQ1nBw4s",
        "colab_type": "code",
        "outputId": "b7a38f6d-26b5-41ae-98e8-aecebf0f13c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_tag(word_tokenize(('I am Sam')))[0][0]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gJHqfo2md8q",
        "colab_type": "text"
      },
      "source": [
        "Détails des significations des tags POS: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYS7IjhYmd8u",
        "colab_type": "text"
      },
      "source": [
        "####  Transformation des données:\n",
        "\n",
        "Classe vide : fonction **à compléter** \n",
        "```python\n",
        "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
        "``` \n",
        "\n",
        "Ne garder que les noms, adverbes, verbes et adjectifs pour notre modèle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LemJxRKmd8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
        "    X_pos = []\n",
        "    for text in X:\n",
        "      tags=pos_tag(word_tokenize(text))\n",
        "      keep =[u[0] for u in  tags if u[1] in good_tags]\n",
        "      tmp=\"\".join(u+\" \" for u in  keep)\n",
        "      X_pos.append(tmp)\n",
        "\n",
        "   \n",
        "    return X_pos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWGOSGHQmd89",
        "colab_type": "code",
        "outputId": "2707fc6e-d06d-43cf-b54d-f91ab9cb959c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "texts_POS = pos_tag_filter(texts_reduced)\n",
        "voc, X = count_words(texts_POS)\n",
        "nb = NB()\n",
        "\n",
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score de classification: 0.7452000000000001 (std 0.0143443368616329)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z67My3eWmd9O",
        "colab_type": "text"
      },
      "source": [
        "## Utilisation d'un classifieur plus complexe ?\n",
        "\n",
        "On peut utiliser les implémentations scikit-learn de classifieurs moins naïfs, comme la régression logistique ou les SVM. Quel en est l'inconvénient principal (imaginons que, plutôt qu'un modèle linéaire, on choisisse d'utiliser un réseau de neurones à plusieurs couches cachées ?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKVbIhk5md9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFDEoRSkmd9X",
        "colab_type": "code",
        "outputId": "067c09ed-745c-4c5b-d850-50862fb212ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pipeline_logistic = Pipeline([\n",
        "   ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "   ('Tf_Idf', TfidfTransformer()),\n",
        "   ('clf', LinearSVC()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_logistic, texts_reduced, y_reduced, cv=5)\n",
        "print(\"LinearSVC Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_logistic = Pipeline([\n",
        "   ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "   ('Tf_Idf', TfidfTransformer()),\n",
        "   ('clf', LogisticRegression()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_logistic, texts_reduced, y_reduced, cv=5)\n",
        "print(\"LogisticRegression Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC Classification score: 0.8272 (std 0.00934665715643831)\n",
            "LogisticRegression Classification score: 0.8231999999999999 (std 0.013302631318652678)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ryTJ2UFmd9f",
        "colab_type": "text"
      },
      "source": [
        "# Représentations denses\n",
        "\n",
        "##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n",
        "\n",
        "**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
        "\n",
        "Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n",
        "\n",
        "**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n",
        " - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
        " - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n",
        "\n",
        "Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n",
        "\n",
        "**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n",
        "- Directement réaliser une analyse sémantique de surface.\n",
        "- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n",
        "\n",
        "\n",
        "**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n",
        "Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCpyIjQcmd9i",
        "colab_type": "text"
      },
      "source": [
        "## Obtenir une représentation: comptes d'occurences et de co-occurences\n",
        "\n",
        "Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf**. On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n",
        "\n",
        "Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n",
        "- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n",
        "*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n",
        "- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n",
        "- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGu9qAZ5md9o",
        "colab_type": "text"
      },
      "source": [
        "#### Procédure\n",
        "\n",
        "Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n",
        "Puis, pour chaque terme $w$ du corpus,\n",
        "- On récupère l'index $i$ correspondant à l'aide de $V$\n",
        "- Pour chaque terme $w'$ du contexte de $w$, \n",
        "  + On récupère l'index $j$ correspondant à l'aide de $V$\n",
        "  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n",
        "  \n",
        "La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n",
        "  \n",
        "#### Exemple\n",
        "\n",
        "On considère le corpus suivant: \n",
        "\n",
        "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
        "\n",
        "On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n",
        "On obtient la matrice suivante: \n",
        "\n",
        "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
        "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
        "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
        "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
        "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
        "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
        "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
        "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
        "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
        "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
        "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noWUqNuTmd9s",
        "colab_type": "text"
      },
      "source": [
        "### Obtenir un Vocabulaire:\n",
        "\n",
        "Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W_teZ1YQG6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "def vocabulary(corpus, count_threshold=1, voc_threshold=0,remove_stopwords=False):\n",
        "    \"\"\"    \n",
        "    Function using word counts to build a vocabulary\n",
        "    Params:\n",
        "        corpus (list of list of strings): corpus of sentences\n",
        "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
        "        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n",
        "    Returns:\n",
        "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
        "                                 values: indexes corresponding to each word sorted by frequency \n",
        "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
        "           \n",
        "                                             values: word counts in the corpus\n",
        "    \"\"\"\n",
        "   \n",
        "    \n",
        "    v = {}\n",
        "    idx=0\n",
        "    for text in corpus:\n",
        "     tokens=  clean_and_tokenize(text)\n",
        "     for token in tokens:\n",
        "       if not token in v.keys():\n",
        "          v[token]=idx\n",
        "          idx+=1\n",
        "    \n",
        "    if remove_stopwords:\n",
        "      clean_v = {}\n",
        "      idx=0\n",
        "      for key in v.keys():\n",
        "        if not key in stopwords.words('english'):\n",
        "            clean_v[key]=idx\n",
        "            idx+=1\n",
        "      v=clean_v\n",
        "    X=np.zeros((len(corpus),len(v)))\n",
        "    for i,text in enumerate(corpus):\n",
        "     tokens=  clean_and_tokenize(text)\n",
        "     for token in tokens:\n",
        "       if token in v.keys():\n",
        "         X[i,v[token]]+=1\n",
        "          \n",
        "    word_counts =dict( ( k,X[:,i].sum() ) for i,k in enumerate(v.keys() )   ) \n",
        "    filtered_word_counts = {}\n",
        "    for key in   word_counts.keys():\n",
        "      if word_counts[key] > count_threshold:\n",
        "               filtered_word_counts[key]=word_counts[key]\n",
        "\n",
        "    \n",
        "    filtered_word_counts=dict(sorted(filtered_word_counts.items(), key=operator.itemgetter(1),reverse=True))\n",
        "   \n",
        " \n",
        "    \n",
        "    vocabulary = {}\n",
        "    vocabulary_word_counts = {} \n",
        "    idx=0\n",
        "    for key in   filtered_word_counts.keys():\n",
        "     if  voc_threshold >0:\n",
        "          if len(vocabulary) < voc_threshold :\n",
        "                vocabulary[key]=idx\n",
        "                \n",
        "                idx+=1\n",
        "                vocabulary_word_counts[key]=filtered_word_counts[key]\n",
        "     else :\n",
        "         vocabulary[key]=idx\n",
        "         idx+=1\n",
        "\n",
        "         vocabulary_word_counts[key]=filtered_word_counts[key]   \n",
        "\n",
        "    \n",
        "    vocabulary['UNK']=idx\n",
        "    vocabulary_word_counts['UNK']=0\n",
        "    return vocabulary, vocabulary_word_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doIpwfPfmd96",
        "colab_type": "code",
        "outputId": "758da9c6-8c49-4f50-db5c-69bc3e3c34cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Example for testing:\n",
        "\n",
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "\n",
        "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
        "\n",
        "voc, counts = vocabulary(corpus)\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'down': 0, 'the': 1, 'UNK': 2}\n",
            "{'down': 6.0, 'the': 6.0, 'UNK': 0}\n",
            "{'down': 0, 'the': 1, 'walked': 2, 'boulevard': 3, 'avenue': 4, 'walk': 5, 'UNK': 6}\n",
            "{'down': 6.0, 'the': 6.0, 'walked': 2.0, 'boulevard': 2.0, 'avenue': 2.0, 'walk': 2.0, 'UNK': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBRiFDymd-F",
        "colab_type": "text"
      },
      "source": [
        "### Obtenir les co-occurences:\n",
        "\n",
        "La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n",
        "Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gou9eaZdmd-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        corpus (list of list of strings): corpus of sentences\n",
        "        vocabulary (dictionary): words to use in the matrix\n",
        "        window (int): size of the context window; when 0, the context is the whole sentence\n",
        "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
        "    Returns:\n",
        "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
        "    \"\"\" \n",
        "    l = len(vocabulary)\n",
        "    M = np.zeros((l,l))\n",
        "    for sent in corpus:\n",
        "        # Obtenir la phrase:\n",
        "        sent =  clean_and_tokenize(sent)\n",
        "        # Obtenir les indexs de la phrase grace au vocabulaire:\n",
        "        \n",
        "        sent_idx = []\n",
        "\n",
        "        for token in sent :\n",
        "          if token in vocabulary:        \n",
        "            sent_idx.append(vocabulary[token])\n",
        "          else :\n",
        "            sent_idx.append(vocabulary[\"UNK\"])\n",
        "            \n",
        "\n",
        "        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n",
        "        for i, idx_i in enumerate(sent_idx):\n",
        "            # On vérifie que le mot est reconnu par le vocabulaire:\n",
        "            if idx_i > -1:\n",
        "                # Si on considère un contexte limité:\n",
        "                if window > 0:\n",
        "                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n",
        "                    l_ctx_idx = []\n",
        "                    s=1\n",
        "                    while s <= window :\n",
        "                      j=i-s\n",
        "                      if j < 0 :\n",
        "                        break;\n",
        "                      else:\n",
        "                         l_ctx_idx.append(sent_idx[j])\n",
        "                         s+=1\n",
        "                # Si on considère que le contexte est la phrase entière:\n",
        "                else:\n",
        "                    # La liste qui contient le contexte à gauche du mot est plus facile à créer:\n",
        "                    l_ctx_idx = []\n",
        "                    for j in range(0,i):\n",
        "                         l_ctx_idx.append(sent_idx[j])\n",
        "                    # A compléter\n",
        "                # On parcourt cette liste et on update M[i,j]:    \n",
        "                for j, idx_j in enumerate(l_ctx_idx):\n",
        "                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n",
        "                    if idx_j > -1:\n",
        "                        # Calcul du poids:\n",
        "                        if distance_weighting:\n",
        "                            weight = 1.0 \n",
        "                            weight/=(j+1)\n",
        "                        else:\n",
        "                            weight = 1.0\n",
        "                        M[idx_i, idx_j] += weight * 1.0\n",
        "                        M[idx_j, idx_i] += weight * 1.0\n",
        "    return M  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXa1e2Hwmd-R",
        "colab_type": "code",
        "outputId": "e45b77d9-828f-4829-b6ba-a585aac0b1c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(co_occurence_matrix(corpus, voc, 0, False))\n",
        "# Attention: les résultats sont différents de l'exemple plus haut car le vocabulaire n'est pas exactement le même: vérifiez par vous même."
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2. 7. 3. 3. 2. 2. 2.]\n",
            " [7. 2. 2. 2. 3. 3. 2.]\n",
            " [3. 2. 0. 1. 1. 0. 0.]\n",
            " [3. 2. 1. 0. 0. 0. 1.]\n",
            " [2. 3. 1. 0. 0. 1. 0.]\n",
            " [2. 3. 0. 0. 1. 0. 1.]\n",
            " [2. 2. 0. 1. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmU0t71Jmd-a",
        "colab_type": "text"
      },
      "source": [
        "#### Application à un vrai jeu de données\n",
        "\n",
        "On va chercher à obtenir ces comptes pour les données **imdb**.\n",
        "\n",
        "#### Etude rapide des données\n",
        "\n",
        "On voudrait ici, avant de procéder, avoir une idée de ce que contiennent ces critiques de films. On va donc obtenir le vocabulaire (entier) et représenter les fréquences des mots, dans l'ordre (attention, il faudra utiliser une échelle logarithmique): on devrait retrouver la loi de Zipf. Cela nous permettra d'avoir une idée de la taille du vocabulaire qu'on pourra choisir : il s'agit de réaliser un compromis entre les ressources nécessaires (taille des objets en mémoire) et quantité d'informations qu'on peut en tirer (les mots rares peuvent apporter beaucoup d'informations, mais il est difficile d'en apprendre de bonnes représentations,car ils sont rares !)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EafwVpLRmd-c",
        "colab_type": "code",
        "outputId": "8f12bd5d-1bff-4346-f542-08c2ad8b5023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "corpus = texts[0::5]\n",
        "\n",
        "vocab, word_counts = vocabulary(corpus,remove_stopwords=True)\n",
        "rank_counts = {w:[vocab[w], word_counts[w]] for w in vocab if vocab[w]}\n",
        "rank_counts_array = np.array(list(rank_counts.values()))\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title('Word counts versus rank')\n",
        "plt.scatter(rank_counts_array[:200,0], rank_counts_array[:200,1])\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "print('Vocabulary size: %i' % len(vocab))\n",
        "print('Part of the corpus by taking the \"x\" most frequent words:')\n",
        "for i in range(5000, len(vocab), 5000):\n",
        "    print('%i : %.2f' % (i, np.sum(rank_counts_array[:i, 1]) / np.sum(rank_counts_array[:,1]) ))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAE/CAYAAAA35xgnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngI4RCAEAAAB0zP8PMN11+80I7bAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 24949\n",
            "Part of the corpus by taking the \"x\" most frequent words:\n",
            "5000 : 0.84\n",
            "10000 : 0.92\n",
            "15000 : 0.96\n",
            "20000 : 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jdZeELImd-k",
        "colab_type": "text"
      },
      "source": [
        "Résultat de l'analyse: on peut se contenter d'un vocabulaire de 10000, voire 5000 mots - c'est important, car cela va déterminer la taille des objets que l'on va manipuler. On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpaxpXA52xhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = texts[::2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdF10GGymd-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_5k, word_counts_5k = vocabulary(corpus, 0, 5000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3s7UgjkaR-O",
        "colab_type": "code",
        "outputId": "2b1c743f-e52c-43d4-b5dd-ed3d9493c811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "M5dist = co_occurence_matrix(corpus, vocab_5k, window=5, distance_weighting=True)\n",
        "M20 = co_occurence_matrix(corpus, vocab_5k, window=20, distance_weighting=False)\n",
        "print(M5dist.shape)\n",
        "print(M20.shape)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5001, 5001)\n",
            "(5001, 5001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu-2t9JYmd-0",
        "colab_type": "code",
        "outputId": "9b76139a-58b2-4292-d59a-725f8a085539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(vocab_5k['cinema'])\n",
        "print(M5dist[429])\n",
        "print(M20[429])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "421\n",
            "[351.73333333  77.98333333  27.81666667 ...   0.           0.\n",
            " 523.46666667]\n",
            "[1.948e+03 8.070e+02 3.050e+02 ... 2.000e+00 1.000e+00 3.869e+03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27kz9rZ-md-7",
        "colab_type": "text"
      },
      "source": [
        "### Comparaison de vecteurs\n",
        "\n",
        "On peut se servir de ces vecteurs de très grande taille pour une analyse sémantique très basique: par exemple, en cherchant les plus proches voisins d'un mot. Cependant, il faudra faire attention aux distances qu'on utilise, liées à certaines métriques (Euclidiennes, Cosine) ou éventuellement d'autres liées à l'appartenance aux ensembles (Matching, Jaccard). La normalisation des vecteurs peut aussi jouer un rôle. Dans tous les cas, il faut bien faire attention à ne pas sur-interprêter ce type de résultats. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyqd84y3md--",
        "colab_type": "code",
        "outputId": "c3d42cd5-be08-448e-c84e-ca925dd5ffbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "def euclidean(u, v):\n",
        "    return np.linalg.norm(u-v)\n",
        "\n",
        "def length_norm(u):\n",
        "    return u / np.sqrt(u.dot(u))\n",
        "\n",
        "def cosine(u, v):\n",
        "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def print_neighbors(distance, voc, co_oc, mot, k=10):\n",
        "    inv_voc = {id: w for w, id in voc.items()}\n",
        "    neigh = NearestNeighbors(k, algorithm='brute', metric=distance)\n",
        "    neigh.fit(co_oc) \n",
        "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n",
        "    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n",
        "    print([[inv_voc[i] for i in s[1:]] for s in ind])\n",
        "    \n",
        "print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n",
        "print_neighbors(euclidean, vocab_5k, M20, 'good')\n",
        "print_neighbors(cosine, vocab_5k, M20, 'good')\n",
        "print(\"\")\n",
        "print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n",
        "print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n",
        "print_neighbors(cosine, vocab_5k, M5dist, 'good') "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avec un contexte large, sans prendre en compte la distance entre les mots:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['very', 'what', 'there', 'more', 'even', 'which', 'only', 'time', 'when']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['very', 'just', 'pretty', 'lot', 'really', 'not', 'decent', 'great', 'actually']]\n",
            "\n",
            "Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['very', 'well', 'even', 'what', 'really', 'also', 'had', 'just', 'great']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'cool', 'well', 'bad', 'decent', 'nice', 'very', 'amazing', 'just']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv2kbi6Umd_L",
        "colab_type": "text"
      },
      "source": [
        "**Normalisation**: Très simple; il s'agit d'annuler l'influence de la magnitude des comptes sur la représentation.\n",
        "\n",
        "$$\\mathbf{m_{normalized}} = \\left[ \n",
        "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "   \\ldots\n",
        "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "\\right]$$\n",
        " \n",
        "**Pointwise Mutual Information**: Il s'agit d'évaluer à quel point la co-occurence des deux termes est *inattendue*. En effet, cette mesure correspond au ratio de la probabilité jointe des deux mots et du produit de leur probabilités individuelles:\n",
        "$$\n",
        "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
        "$$\n",
        "La probabilité jointe des deux mots correspond au nombre de fois ou on les observe ensemble, divisé par le nombre total de co-occurences du corpus: \n",
        "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
        "La probabilité individuelle d'un mot correspond simplement à sa fréquence, que l'on peut calculer en comptant toutes les co-occurences ou ce mot apparaît:\n",
        "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
        "Ainsi,\n",
        "$$ \n",
        "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
        "$$\n",
        "On calcule ainsi le décalage entre l'observation que l'on a fait dans notre corpus et la fréquence d'apparition de ces termes si on les considère indépendant - c'est à dire qu'on suppose que leur co-occurence est une coïncidence.\n",
        "\n",
        "Le principal problème avec cette mesure est qu'elle n'est pas adaptée au cas où l'on observe aucune co-occurence. Puisque la PMI est censée renvoyer une quantité positive si l'on observe plus de co-occurences que prévu, et négative si l'on en observe moins, on ne peut pas choisir de remplacer $\\log(0)$ par $0$. Une solution couramment utilisée est d'utiliser la **Positive PMI**, qui fixe toutes les valeurs négatives à $0$.\n",
        " \n",
        " $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
        " \\begin{cases}\n",
        " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
        " 0 & \\textrm{otherwise}\n",
        " \\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m6IMB8xmd_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pmi(co_oc, positive=True):\n",
        "    sum_vec = co_oc.sum(axis=0)\n",
        "    sum_tot = sum_vec.sum()\n",
        "    with np.errstate(divide='ignore'):\n",
        "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
        "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
        "    if positive:\n",
        "        pmi[pmi < 0] = 0.0\n",
        "    return pmi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZWLdrKpmd_h",
        "colab_type": "code",
        "outputId": "55102ff9-2fbd-4b3b-a33b-9359978b8dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "PPMI5 = pmi(M5dist)\n",
        "PPMI20 = pmi(M20)\n",
        "\n",
        "print(\"Avec la PPMI:\")    \n",
        "print_neighbors(euclidean, vocab_5k, PPMI5, 'good')\n",
        "print_neighbors(cosine, vocab_5k, PPMI5, 'good')\n",
        "print_neighbors(euclidean, vocab_5k, PPMI20, 'good')\n",
        "print_neighbors(cosine, vocab_5k, PPMI20, 'good')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avec la PPMI:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['UNK', 'movie', 'but', 'this', 'bad', 'the', 'film', 'that', 'not']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['bad', 'great', 'decent', 'excellent', 'funny', 'nice', 'but', 'interesting', 'fine']]\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['but', 'and', 'the', 'this', 'movie', 'that', 'not', 'UNK', 'really']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['but', 'great', 'bad', 'decent', 'really', 'acting', 'movie', 'pretty', 'liked']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09dOxKHwmd_r",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF**: Comme on l'a déjà vu, il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF). \n",
        "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice $termes \\times documents$. Ainsi, pour une matrice $\\mathbf{X}$ de $n$ termes et $d$ documents: \n",
        "\n",
        " $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n",
        " \n",
        " $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n",
        " \n",
        " $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n",
        "\n",
        "\n",
        "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents n'est à priori pas optimal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtNG-MBtmd_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf(co_oc):\n",
        "    \"\"\"\n",
        "    Inverse document frequencies applied to our co_oc matrices\n",
        "    \"\"\"\n",
        "    # IDF\n",
        "    d = float(co_oc.shape[1])\n",
        "    in_doc = co_oc.astype(bool).sum(axis=1)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        idfs = np.log(d / in_doc)\n",
        "    idfs[np.isinf(idfs)] = 0.0  # log(0) = 0\n",
        "    # TF\n",
        "    sum_vec = co_oc.sum(axis=0)\n",
        "    tfs = co_oc / sum_vec\n",
        "    return (tfs.T * idfs).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIqcdkumd_1",
        "colab_type": "code",
        "outputId": "76fcf07b-5434-40f6-fc31-729c2855d315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "TFIDF5 = tfidf(M5dist)\n",
        "\n",
        "print(\"Avec TF-IDF:\")    \n",
        "print_neighbors(euclidean, vocab_5k, TFIDF5, 'good')\n",
        "print_neighbors(cosine, vocab_5k, TFIDF5, 'good')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avec TF-IDF:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['movie', 'not', 'film', 'are', 'but', 'its', 'was', 'one', 'has']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['but', 'this', 'that', 'movie', 'great', 'not', 'its', 'was', 'some']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG9oH2IRmd_-",
        "colab_type": "text"
      },
      "source": [
        "### Matrice de co-occurences : Réduction de dimension\n",
        "\n",
        "#### Motivation\n",
        "\n",
        "Il s'agit non seulement de réduire la taille de données (ainsi, on traitera des vecteurs de dimension réduite, plutôt que de travailler avec des vecteurs de la taille du vocabulaire) mais aussi de mettre en évidence des relations de plus haut niveau entre les mots: en réduisant leurs représentations aux dimensions qui *les plus importantes* des données, on se retrouve à *généraliser* certaines propriétés entre les mots.\n",
        "\n",
        "#### Réduction de dimension via SVD \n",
        "\n",
        "Une matrice est une transformation linéaire: y appliquer une SVD, c'est décomposer notre transformation linéaire en un produit de transformations linéaires de différents types. Il va s'agir d'effectuer un changement de base, et de replacer nos données dans un espace ou chacune des coordonnées sont inchangées par la transformation effectuée. Ainsi, on décompose la matrice $\\mathbf{M}$ en trois matrices:\n",
        "\n",
        "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
        "\n",
        "Les matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ ont les propriétés suivantes:\n",
        "- $\\mathbf{U}$ et $\\mathbf{V}$ sont des matrices orthogonales ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ et $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). Elles contiennent les vecteurs propres à gauche et à droite de $\\mathbf{M}$.\n",
        "- $\\mathbf{\\lambda}$ est une matrice diagonale: attention, elle n'est pas forcément carrée. Les coefficients de la diagonale sont les valeurs propres de $\\mathbf{M}$.\n",
        "\n",
        "Ainsi, les dmensions *les plus importantes* correspondent aux plus grandes valeurs propres. Réduire nos données à une dimension $k$ correspond à ne garder que les vecteurs correspondant aux $k$ premières valeurs propres - et cela revient à prendre les $k$ premiers vecteurs de la matrice $U$. \n",
        "On utilise ici ```TruncatedSVD``` du package ```scikit-learn```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22AZ2iAKmeAB",
        "colab_type": "code",
        "outputId": "bfd5b301-3444-407d-8fb8-ee1f19b3ca63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=300)\n",
        "SVDEmbeddings = svd.fit_transform(M5dist)\n",
        "print(SVDEmbeddings.shape)\n",
        "SVDEmbeddings[vocab_5k['UNK']]\n",
        "\n",
        "print_neighbors(euclidean, vocab_5k, SVDEmbeddings, 'good')\n",
        "print_neighbors(cosine, vocab_5k, SVDEmbeddings, 'good')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5001, 300)\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['very', 'well', 'even', 'what', 'really', 'also', 'had', 'just', 'great']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'cool', 'decent', 'well', 'lame', 'nice', 'bad', 'amazing', 'impressive']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stkNyRD1meAj",
        "colab_type": "text"
      },
      "source": [
        "Note: Lorsque l'on applique cette méthode à la matrice des comptes $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$, on obtient la méthode appellée **Latent Semantic Analysis**, pour la détection de composantes latentes (sémantiques) permettant de regrouper les documents.  \n",
        "\n",
        "#### Visualisation en deux dimensions\n",
        "\n",
        "On va maintenant utiliser **l'analyse en composantes principales** (PCA) pour visualiser nos données en 2 dimensions.  Cela revient à appliquer la SVD à la matrice de covariance des données, pour que les directions principales soient indépendantes les unes des autres et maximisent la variance des données.\n",
        "On utilise la classe ```PCA``` du package ```scikit-learn```: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI6UeLz4meAk",
        "colab_type": "code",
        "outputId": "36ec1b1f-19e9-4f5f-cc8f-cee7ec018906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, whiten=True)\n",
        "Emb = pca.fit_transform(M5dist)\n",
        "\n",
        "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
        "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
        "         'film', 'movie', 'oscar', 'award']\n",
        "ind_words = [vocab_5k[w] for w in words]\n",
        "x_words = [Emb[ind,0] for ind in ind_words]\n",
        "y_words = [Emb[ind,1] for ind in ind_words]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_words, y_words)\n",
        "\n",
        "for i, w in enumerate(words):\n",
        "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neR/V1C9UAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrVhm3wImeAo",
        "colab_type": "code",
        "outputId": "e615e1ed-5803-460b-ee41-51db9966f53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n",
        "\n",
        "pca = PCA(n_components=2, whiten=True)\n",
        "Emb = pca.fit_transform(Norm5)\n",
        "\n",
        "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
        "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
        "         'film', 'movie', 'oscar', 'award']\n",
        "ind_words = [vocab_5k[w] for w in words]\n",
        "x_words = [Emb[ind,0] for ind in ind_words]\n",
        "y_words = [Emb[ind,1] for ind in ind_words]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_words, y_words)\n",
        "\n",
        "for i, w in enumerate(words):\n",
        "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSMVGRUMmeAz",
        "colab_type": "text"
      },
      "source": [
        "## Obtenir une représentation: algorithmes couramment utilisés\n",
        "\n",
        "L'idée, ici, est de définir un ensemble de représentations ${w_{i}}_{i=1}^{V}$, de dimension prédéfinie $d$ (ici, on travaillera avec $d = 300$), pour tous les mots $i$ du vocabulaire $V$ - puis **d'entraîner** ces représentations pour qu'elles correspondent à ce que l'on souhaite. \n",
        "\n",
        "### Glove\n",
        "\n",
        "L'objectif défini par Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) est d'apprendre des vecteurs $w_{i}$ et $w_{k}$ de façon à ce que leur produit scalaire correspondent au logarithme de leur **Pointwise Mutual Information**: \n",
        "\n",
        "\n",
        "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
        "\n",
        "\n",
        "Dans l'article, l'obtention de cet objectif est minutieusement justifié par un raisonnement sur les opérations que l'on veut effectuer avec ces vecteurs et les propriétés qu'ils devraient avoir - notamment, une symétrie entre les lignes et les colonnes (voir l'article pour plus de détails).  \n",
        "L'objectif final obtenu est le suivant, où $M$ est la matrice de co-occurences:\n",
        "\n",
        "\n",
        "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
        "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
        "  \n",
        " \n",
        "Ici, $f$ est une fonction de *mise à l'échelle* qui permet de diminuer l'importance des comptes de co-occurences les plus fréquents: \n",
        "\n",
        "\n",
        "$$f(x) \n",
        "\\begin{cases}\n",
        "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
        "1 & \\textrm{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "\n",
        "En général, on choisit $\\alpha=0.75$ et $x_{\\max} = 100$, même si ces paramètres peuvent nécessiter un changement selon les données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjfDYfRtmeA2",
        "colab_type": "text"
      },
      "source": [
        "Le code suivant utilise l'API de gensim pour récupérer des représentations pré-entrainées (Il est normal que le chargement soit long)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beQ4SMJZmeA5",
        "colab_type": "code",
        "outputId": "da290293-cac0-4368-a2f0-51522349836c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KViNnZJtmeA_",
        "colab_type": "text"
      },
      "source": [
        "On peut extraire la matrice des embeddings ainsi, et vérifier sa taille:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tiy24PYgmeBC",
        "colab_type": "code",
        "outputId": "48ffe1f1-c1eb-444a-93aa-d6e2efdcd845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_glove_embeddings = loaded_glove_model.vectors\n",
        "print(loaded_glove_embeddings.shape)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq8KRdo8meBI",
        "colab_type": "text"
      },
      "source": [
        "On voit donc qu'il y a $400.000$ mots représentés, et que les embeddings sont de dimension $300$. On définit une fonction qui nous renvoie, à partir du modèle chargé, le vocabulaire et la matrice des embeddings suivant les structures que l'on a utilisé auparavant. On ajoute, ici encore, un mot inconnu ```'UNK'``` au cas où se trouve dans nos données des mots qui ne font pas parti des $400.000$ mots représentés ici. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5n5TJFumeBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_glove_voc_and_embeddings(glove_model):\n",
        "    voc = {word : index for word, index in enumerate(glove_model.index2word)}\n",
        "    voc['UNK'] = len(voc)\n",
        "    embeddings = glove_model.vectors\n",
        "    return voc, embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78s7bKPPmeBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YtNwxMKmeBj",
        "colab_type": "text"
      },
      "source": [
        "Afin de comparer 'à jeu égal' les représentations chargées ici et celles que l'on a produite, il faudrait utiliser le même vocabulaire. Dans ce but, je réutilise le code qui suit pour créer un vocabulaire de $5000$ mots à partir des données exactement comme hier, et j'ajoute à la fin une fonction qui renvoie la matrices des représentations chargées avec Glove pour ces $5000$ mots seulement, dans le bon ordre. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoPCSr5GmeBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_glove_adapted_embeddings(glove_model, input_voc):\n",
        "    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n",
        "    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n",
        "    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n",
        "    for i, ind in index_dict.items():\n",
        "        embeddings[i] = glove_model.vectors[ind]\n",
        "    return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uoLgCHrmeBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdGZ5IE-meBx",
        "colab_type": "text"
      },
      "source": [
        "Cette fonction prend donc en entrée le modèle chargé à l'aide de l'API Gensim, ainsi qu'un vocabulaire que nous avons créé nous même, et renvoie la matrice d'embeddings tiré du modèle chargé, pour les mots notre vocabulaire et dans le bon ordre.\n",
        "Remarque: les mots inconnus sont représentés par le vecteur nul:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akpbUbeDmeBz",
        "colab_type": "code",
        "outputId": "0550f916-72cd-4fd2-95ca-bd4085c37944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "print(GloveEmbeddings.shape)\n",
        "GloveEmbeddings[vocab_5k['UNK']]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5001, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gz9AqCfmeB4",
        "colab_type": "code",
        "outputId": "97ed2dfd-1a8f-48fc-f186-dacc462e6348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n",
        "print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['better', 'well', 'always', 'really', 'sure', 'way', 'but', 'excellent', 'certainly']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMznPh77meB9",
        "colab_type": "text"
      },
      "source": [
        "## Application à l'analyse de sentiments\n",
        "\n",
        "On va maintenant utiliser ces représentations pour l'analyse de sentiments. \n",
        "Le modèle de base, comme hier, sera construit en deux étapes:\n",
        "- Une fonction permettant d'obtenir des représentations vectorielles des critiques, à partir des textes, du vocabulaire, et des représentations vectorielles des mots. Une telle fonction (à compléter ci-dessous) va associer à chaque mot d'une critique son embeddings, et créer la représentation pour l'ensemble de la phrase en sommant ces embeddings.\n",
        "- Un classifieur qui prendra ces représentations en entrée et réalisera une prédiction. Pour le réaliser, on pourra utiliser d'abord la régression logistique ```LogisticRegression``` de ```scikit-learn```  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYjJ7WxOmeB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
        "    \"\"\"\n",
        "    Represent the sentences as a combination of the vector of its words.\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : a list of sentences   \n",
        "    vocabulary : dict\n",
        "        From words to indexes of vector.\n",
        "    embeddings : Matrix containing word representations\n",
        "    np_func : function (default: np.sum)\n",
        "        A numpy matrix operation that can be applied columnwise, \n",
        "        like `np.mean`, `np.sum`, or `np.prod`. \n",
        "    Returns\n",
        "    -------\n",
        "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
        "    \"\"\"\n",
        "    #\n",
        "    # A compléter !\n",
        "    # \n",
        "    representations=np.zeros((len(texts), embeddings.shape[1]))\n",
        "    for i,text in enumerate(texts):\n",
        "      tokens=clean_and_tokenize(text)\n",
        "      em=[]\n",
        "      for token in tokens:\n",
        "\n",
        "        if token in vocabulary.keys():\n",
        "         idx=vocabulary[token]\n",
        "\n",
        "        else :\n",
        "          idx=vocabulary[\"UNK\"]\n",
        "      \n",
        "        \n",
        "        em.append(embeddings[idx])\n",
        "      representations[i]=np_func(em,axis=0)\n",
        "    return representations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUg1AGJYmeCF",
        "colab_type": "code",
        "outputId": "db124979-5025-40fc-e35d-9777f8daf2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Exemple avec les embeddings obtenus via Glove\n",
        "rep = sentence_representations(corpus, vocab_5k, GloveEmbeddings)\n",
        "y=y[::2]\n",
        "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
        "print(clf.score(rep[1::2], y[1::2]))\n",
        "\n",
        "scores = cross_val_score(clf, rep, y, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.82512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score de classification: 0.8231999999999999 (std 0.006630837051232663)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NixC60BmeCJ",
        "colab_type": "text"
      },
      "source": [
        "Vous pouvez maintenant comparer l'ensemble des méthodes, et notamment répondre aux questions suivantes:\n",
        "- Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Glove soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n",
        "- Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCsRB2E0M9sm",
        "colab_type": "text"
      },
      "source": [
        "## Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VHu28iZuVPX",
        "colab_type": "text"
      },
      "source": [
        "- Glove is learned on a larger corpus so we have a better result for the embeddings\n",
        "- To compare Glove in a fair way with the other methods of learning representations it will be necessary to retrain glove on the same corpus as the other methods to be able to compare the performances\n",
        "- In term of performance the PPMI Représentations combine with SVD give the best performance with comparison to other method"
      ]
    }
  ]
}