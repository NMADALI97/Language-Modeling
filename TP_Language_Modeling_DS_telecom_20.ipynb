{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "TP_Language_Modeling_DS-telecom-20.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kU_xRXAg37zF"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eeumoy9T8rlH",
        "colab_type": "code",
        "outputId": "855c6148-4d19-42a1-b902-38f05dd967d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1rJrn1oeB_7N5YaF2a0T2QsKRfurejnOW\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rJrn1oeB_7N5YaF2a0T2QsKRfurejnOW\n",
            "To: /content/data.zip\n",
            "5.22MB [00:01, 3.96MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATmSrrM_37uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhtctFKY37vN",
        "colab_type": "text"
      },
      "source": [
        "### A (very small) introduction to pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUbgxRQ637vP",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Tensors are very similar to Numpy arrays, with the added benefit of being usable on GPU. For a short tutorial on various methods to create tensors of particular types, see [this link](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n",
        "The important things to note are that Tensors can be created empty, from lists, and it is very easy to convert a numpy array into a pytorch tensor, and inversely.\n",
        "One very important way to manipulate tensors that is different from numpy is the method ```.view``` which is used, as ```reshape```, to change the shape of a tensor. The difference is that ```.view``` will avoid making a copy of the tensor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS8RKsSk37vc",
        "colab_type": "code",
        "outputId": "84d549f9-4df0-4275-d200-9ca93ad8a73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "a = torch.LongTensor(5)\n",
        "b = torch.LongTensor([5])\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([           71702784,                  27,          4294967295,\n",
            "        4908972153413002606, 7214836307739637349])\n",
            "tensor([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byUCQ5Az37vs",
        "colab_type": "code",
        "outputId": "eb073dee-7735-45e5-a45a-f27913de81f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = torch.FloatTensor([2])\n",
        "b = torch.FloatTensor([3])\n",
        "\n",
        "print(a + b)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh1ZLq1737v6",
        "colab_type": "text"
      },
      "source": [
        "The main interest in us using Pytorch is the ```autograd``` package. ```torch.Tensor```objects have an attribute ```.requires_grad```; if set as True, it starts to track all operations on it. When you finish your computation, can call ```.backward()``` and all the gradients are computed automatically (and stored in the ```.grad``` attribute).\n",
        "\n",
        "One way to easily cut a tensor from the computational once it is not needed anymore is to use ```.detach()```.\n",
        "More info on automatic differentiation in pytorch on [this link](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-b16lsL37v-",
        "colab_type": "code",
        "outputId": "0bdba89d-dea9-49c9-dbac-2fdd0ed18a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "x = torch.tensor(1., requires_grad=True)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build a computational graph.\n",
        "y = w * x + b    # y = 2 * x + 3\n",
        "\n",
        "# Compute gradients.\n",
        "y.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad)    # x.grad = 2 \n",
        "print(w.grad)    # w.grad = 1 \n",
        "print(b.grad)    # b.grad = 1 "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-bz_TbD37wN",
        "colab_type": "code",
        "outputId": "7b5d5a9c-71f4-4339-bce8-6af949aba2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "x = torch.randn(10, 3)\n",
        "y = torch.randn(10, 2)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(3, 2)\n",
        "for name, p in linear.named_parameters():\n",
        "    print(name)\n",
        "    print(p)\n",
        "\n",
        "# Build loss function - Mean Square Error\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Forward pass.\n",
        "pred = linear(x)\n",
        "\n",
        "# Compute loss.\n",
        "loss = criterion(pred, y)\n",
        "print('Initial loss: ', loss.item())\n",
        "\n",
        "# Backward pass.\n",
        "loss.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print ('dL/dw: ', linear.weight.grad) \n",
        "print ('dL/db: ', linear.bias.grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight\n",
            "Parameter containing:\n",
            "tensor([[-0.4742, -0.0415,  0.4474],\n",
            "        [ 0.1669,  0.0388,  0.5498]], requires_grad=True)\n",
            "bias\n",
            "Parameter containing:\n",
            "tensor([ 0.5560, -0.1842], requires_grad=True)\n",
            "Initial loss:  1.3143914937973022\n",
            "dL/dw:  tensor([[-0.5299, -0.1074,  0.4377],\n",
            "        [ 0.4601, -0.1714,  0.1747]])\n",
            "dL/db:  tensor([-0.1610, -0.0295])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCPLC1iP37wZ",
        "colab_type": "code",
        "outputId": "d110ec75-057b-466d-d976-90eeb5399a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# You can perform gradient descent manually, with an in-place update ...\n",
        "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
        "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
        "\n",
        "# Print out the loss after 1-step gradient descent.\n",
        "pred = linear(x)\n",
        "loss = criterion(pred, y)\n",
        "print('Loss after one update: ', loss.item())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after one update:  1.3066240549087524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WhRhr7d37wn",
        "colab_type": "code",
        "outputId": "fe9d0553-81db-4b3e-e349-e16a5ca63e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use the optim package to define an Optimizer that will update the weights of the model.\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# By default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "# is called. Before the backward pass, we need to use the optimizer object to zero all of the\n",
        "# gradients.\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "# Calling the step function on an Optimizer makes an update to its parameters\n",
        "optimizer.step()\n",
        "\n",
        "# Print out the loss after the second step of gradient descent.\n",
        "pred = linear(x)\n",
        "loss = criterion(pred, y)\n",
        "print('Loss after two updates: ', loss.item())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after two updates:  1.2990766763687134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtrfU06-37wy",
        "colab_type": "text"
      },
      "source": [
        "### Tools for data processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQkQcohf37w2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PPTgmqC37xL",
        "colab_type": "text"
      },
      "source": [
        "We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n",
        "- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n",
        "- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n",
        "- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n",
        "- The ```total``` count of words in the dictionary.\n",
        "\n",
        "Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmLqQmq837xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = {}\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            self.counter.setdefault(word, 0)\n",
        "        self.counter[word] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vextLcgZ37xe",
        "colab_type": "code",
        "outputId": "284b7d8f-a842-4c3a-e01c-2f0754b54a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "with open('wikitext-2/train.txt', 'r') as f:\n",
        "    print(f.readline())\n",
        "    print(f.readline())\n",
        "    print(f.readline())\n",
        "    print(f.readline())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "\n",
            " = Valkyria Chronicles III = \n",
            "\n",
            " \n",
            "\n",
            " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPbeRUh637xr",
        "colab_type": "code",
        "outputId": "9ba4da26-c72a-462f-eacc-8674b16a8ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's take the four first lines of our training data:\n",
        "corpus = ''\n",
        "with open('wikitext-2/train.txt', 'r') as f:\n",
        "    for i in range(4):\n",
        "        corpus += f.readline()\n",
        "        \n",
        "# Create an empty Dictionary, separate and add all words. \n",
        "dictio = Dictionary()\n",
        "words = corpus.split()\n",
        "for word in words:\n",
        "    dictio.add_word(word)\n",
        "\n",
        "# Take a look at the objects created:\n",
        "pp.pprint(dictio.word2idx)\n",
        "pp.pprint(dictio.idx2word)\n",
        "pp.pprint(dictio.counter)\n",
        "pp.pprint(dictio.total)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\"': 60,\n",
            " '(': 9,\n",
            " ')': 18,\n",
            " ',': 12,\n",
            " '.': 14,\n",
            " '2011': 44,\n",
            " '3': 6,\n",
            " ':': 7,\n",
            " '<unk>': 8,\n",
            " '=': 0,\n",
            " '@-@': 29,\n",
            " 'Battlefield': 17,\n",
            " 'Chronicles': 2,\n",
            " 'Europan': 70,\n",
            " 'Gallia': 67,\n",
            " 'III': 3,\n",
            " 'Imperial': 80,\n",
            " 'January': 43,\n",
            " 'Japan': 24,\n",
            " 'Japanese': 10,\n",
            " 'Media.Vision': 37,\n",
            " 'Nameless': 61,\n",
            " 'PlayStation': 39,\n",
            " 'Portable': 40,\n",
            " 'Raven': 81,\n",
            " 'Released': 41,\n",
            " 'Second': 69,\n",
            " 'Sega': 35,\n",
            " 'Senjō': 4,\n",
            " 'Valkyria': 1,\n",
            " 'War': 71,\n",
            " 'a': 26,\n",
            " 'against': 79,\n",
            " 'and': 36,\n",
            " 'are': 77,\n",
            " 'as': 22,\n",
            " 'black': 75,\n",
            " 'by': 34,\n",
            " 'commonly': 19,\n",
            " 'developed': 33,\n",
            " 'during': 68,\n",
            " 'first': 58,\n",
            " 'follows': 59,\n",
            " 'for': 38,\n",
            " 'fusion': 49,\n",
            " 'game': 32,\n",
            " 'gameplay': 52,\n",
            " 'in': 42,\n",
            " 'is': 25,\n",
            " 'it': 45,\n",
            " 'its': 53,\n",
            " 'lit': 13,\n",
            " 'military': 63,\n",
            " 'nation': 66,\n",
            " 'no': 5,\n",
            " 'of': 15,\n",
            " 'operations': 76,\n",
            " 'outside': 23,\n",
            " 'parallel': 57,\n",
            " 'penal': 62,\n",
            " 'perform': 73,\n",
            " 'pitted': 78,\n",
            " 'playing': 30,\n",
            " 'predecessors': 54,\n",
            " 'real': 50,\n",
            " 'referred': 20,\n",
            " 'role': 28,\n",
            " 'runs': 56,\n",
            " 'same': 48,\n",
            " 'secret': 74,\n",
            " 'series': 47,\n",
            " 'serving': 65,\n",
            " 'story': 55,\n",
            " 'tactical': 27,\n",
            " 'the': 16,\n",
            " 'third': 46,\n",
            " 'time': 51,\n",
            " 'to': 21,\n",
            " 'unit': 64,\n",
            " 'video': 31,\n",
            " 'who': 72,\n",
            " '戦場のヴァルキュリア3': 11}\n",
            "['=',\n",
            " 'Valkyria',\n",
            " 'Chronicles',\n",
            " 'III',\n",
            " 'Senjō',\n",
            " 'no',\n",
            " '3',\n",
            " ':',\n",
            " '<unk>',\n",
            " '(',\n",
            " 'Japanese',\n",
            " '戦場のヴァルキュリア3',\n",
            " ',',\n",
            " 'lit',\n",
            " '.',\n",
            " 'of',\n",
            " 'the',\n",
            " 'Battlefield',\n",
            " ')',\n",
            " 'commonly',\n",
            " 'referred',\n",
            " 'to',\n",
            " 'as',\n",
            " 'outside',\n",
            " 'Japan',\n",
            " 'is',\n",
            " 'a',\n",
            " 'tactical',\n",
            " 'role',\n",
            " '@-@',\n",
            " 'playing',\n",
            " 'video',\n",
            " 'game',\n",
            " 'developed',\n",
            " 'by',\n",
            " 'Sega',\n",
            " 'and',\n",
            " 'Media.Vision',\n",
            " 'for',\n",
            " 'PlayStation',\n",
            " 'Portable',\n",
            " 'Released',\n",
            " 'in',\n",
            " 'January',\n",
            " '2011',\n",
            " 'it',\n",
            " 'third',\n",
            " 'series',\n",
            " 'same',\n",
            " 'fusion',\n",
            " 'real',\n",
            " 'time',\n",
            " 'gameplay',\n",
            " 'its',\n",
            " 'predecessors',\n",
            " 'story',\n",
            " 'runs',\n",
            " 'parallel',\n",
            " 'first',\n",
            " 'follows',\n",
            " '\"',\n",
            " 'Nameless',\n",
            " 'penal',\n",
            " 'military',\n",
            " 'unit',\n",
            " 'serving',\n",
            " 'nation',\n",
            " 'Gallia',\n",
            " 'during',\n",
            " 'Second',\n",
            " 'Europan',\n",
            " 'War',\n",
            " 'who',\n",
            " 'perform',\n",
            " 'secret',\n",
            " 'black',\n",
            " 'operations',\n",
            " 'are',\n",
            " 'pitted',\n",
            " 'against',\n",
            " 'Imperial',\n",
            " 'Raven']\n",
            "{'\"': 4,\n",
            " '(': 1,\n",
            " ')': 1,\n",
            " ',': 6,\n",
            " '.': 4,\n",
            " '2011': 1,\n",
            " '3': 2,\n",
            " ':': 2,\n",
            " '<unk>': 3,\n",
            " '=': 2,\n",
            " '@-@': 2,\n",
            " 'Battlefield': 1,\n",
            " 'Chronicles': 3,\n",
            " 'Europan': 1,\n",
            " 'Gallia': 1,\n",
            " 'III': 2,\n",
            " 'Imperial': 1,\n",
            " 'January': 1,\n",
            " 'Japan': 2,\n",
            " 'Japanese': 1,\n",
            " 'Media.Vision': 1,\n",
            " 'Nameless': 1,\n",
            " 'PlayStation': 1,\n",
            " 'Portable': 1,\n",
            " 'Raven': 1,\n",
            " 'Released': 1,\n",
            " 'Second': 1,\n",
            " 'Sega': 1,\n",
            " 'Senjō': 1,\n",
            " 'Valkyria': 5,\n",
            " 'War': 1,\n",
            " 'a': 2,\n",
            " 'against': 1,\n",
            " 'and': 4,\n",
            " 'are': 1,\n",
            " 'as': 2,\n",
            " 'black': 1,\n",
            " 'by': 1,\n",
            " 'commonly': 1,\n",
            " 'developed': 1,\n",
            " 'during': 1,\n",
            " 'first': 1,\n",
            " 'follows': 1,\n",
            " 'for': 1,\n",
            " 'fusion': 1,\n",
            " 'game': 3,\n",
            " 'gameplay': 1,\n",
            " 'in': 3,\n",
            " 'is': 2,\n",
            " 'it': 1,\n",
            " 'its': 1,\n",
            " 'lit': 1,\n",
            " 'military': 1,\n",
            " 'nation': 1,\n",
            " 'no': 1,\n",
            " 'of': 3,\n",
            " 'operations': 1,\n",
            " 'outside': 1,\n",
            " 'parallel': 1,\n",
            " 'penal': 1,\n",
            " 'perform': 1,\n",
            " 'pitted': 1,\n",
            " 'playing': 1,\n",
            " 'predecessors': 1,\n",
            " 'real': 1,\n",
            " 'referred': 1,\n",
            " 'role': 1,\n",
            " 'runs': 1,\n",
            " 'same': 1,\n",
            " 'secret': 1,\n",
            " 'series': 1,\n",
            " 'serving': 1,\n",
            " 'story': 1,\n",
            " 'tactical': 2,\n",
            " 'the': 11,\n",
            " 'third': 1,\n",
            " 'time': 1,\n",
            " 'to': 2,\n",
            " 'unit': 2,\n",
            " 'video': 1,\n",
            " 'who': 1,\n",
            " '戦場のヴァルキュリア3': 1}\n",
            "132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDqapbH537x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "                tokens += len(words)\n",
        "        \n",
        "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
        "        ids=[]\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                \n",
        "        return torch.from_numpy(np.array(ids).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZbTt9Fh37yI",
        "colab_type": "text"
      },
      "source": [
        "We use the corpus [wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/). While it's a small dataset, we will need to reduce it if we want to train a model on it without a gpu. With the 'small' version, on most computers, the model model should see one epoch of the data in less than 15 minutes on cpu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK90yYpW37yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "data = 'wikitext-2-small/'\n",
        "corpus = Corpus(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RxKHXg437yW",
        "colab_type": "code",
        "outputId": "cd6bbbea-6eb7-4a91-f62c-6f27c05f2b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#Examples and visualization\n",
        "print(corpus.dictionary.total)\n",
        "print(len(corpus.dictionary.idx2word))\n",
        "print(len(corpus.dictionary.word2idx))\n",
        "\n",
        "print(corpus.train.shape)\n",
        "print(corpus.train[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n",
        "\n",
        "print(corpus.valid.shape)\n",
        "print(corpus.valid[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383196\n",
            "19482\n",
            "19482\n",
            "torch.Size([275485])\n",
            "tensor([0, 1, 2, 3, 4, 1, 0])\n",
            "['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n",
            "torch.Size([47945])\n",
            "tensor([    0,     1, 17642, 17643,     1,     0,     0])\n",
            "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5yXQrz937yh",
        "colab_type": "text"
      },
      "source": [
        "We now have data under a very long list of indexes: the text is as one sequence.\n",
        "Note that this is absolutely not the best way to proceed with large quantities of data (where we'll try not to store huge tensors in memory but read them from file as we go) !\n",
        "But here, we are looking for simplicity and efficiency with regards to computation time.\n",
        "That is why we will ignore sentence separations and treat the data as one long stream that we will cut arbitrarily as we need.\n",
        "\n",
        "\n",
        "The idea now is to create batches from this.\n",
        "With the alphabet being our data, we currently have the sequence:\n",
        "$$ \\left[ \\text{ a b c d e f g h i j k l m n o p q r s t u v w x y z } \\right] $$\n",
        "We want to reorganize it as independant batches that can be processed in parallel by the model !\n",
        "For instance, with the alphabet as the sequence and batch size 4, we'd get a batch of the 4 following sequences of the same length (6 letters):\n",
        "$$ \n",
        "\\begin{bmatrix}\n",
        "\\text{a} & \\text{g} & \\text{m} & \\text{s} \\\\\n",
        "\\text{b} & \\text{h} & \\text{n} & \\text{t} \\\\\n",
        "\\text{c} & \\text{i} & \\text{o} & \\text{u} \\\\\n",
        "\\text{d} & \\text{j} & \\text{p} & \\text{v} \\\\\n",
        "\\text{e} & \\text{k} & \\text{q} & \\text{w} \\\\\n",
        "\\text{f} & \\text{l} & \\text{r} & \\text{x}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "with the last two elements being lost.\n",
        "Again, these columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing. The function ```batchify``` will allow us to reorganize the data as such, and if possible put it on the GPU. We need to cut the unnecessary elements, and put the data in the right shape.\n",
        "\n",
        "\n",
        "**Important**: You can notice that the data is ordered along the columns, which is unusual since it is the second dimension. To do so, you will need to transpose the matrix at some point. While it may not be how we usually organize batches, it will be useful when dealing with LSTMs and their particular organization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZnrNk9137ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchify(data, batch_size, cuda = False):\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Cut the elements that are unnecessary - use the method 'narrow'\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Reorganize the data - use the method 'view'\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    # If we can use a GPU, let's tranfer the tensor to it\n",
        "    if cuda:\n",
        "        data = data.cuda()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAOvw-RJ37yx",
        "colab_type": "text"
      },
      "source": [
        "We now have a way to divide our data into parallel batches. However, our network will not be able to process sequences of arbitrary length !\n",
        "We will then have to define a maximum length that sequences can have, and cut batches along their first dimension (which is the temporal dimension, as words are ordered this way)\n",
        "\n",
        "\n",
        "The function ```get_batch``` subdivides the source data into chunks of the appropriate length.\n",
        "It also separates the source data into the **input** and the **output** of the network. (Remember: we want to predict the next word, so the output is the input shifted by one step in the temporal axis).\n",
        "If ```source``` is equal to the example output of the batchify function, with a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "$$ \n",
        "\\begin{bmatrix}\n",
        "\\text{a} & \\text{g} & \\text{m} & \\text{s} \\\\\n",
        "\\text{b} & \\text{h} & \\text{n} & \\text{t} \\\\\n",
        "\\text{c} & \\text{i} & \\text{o} & \\text{u} \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$ \n",
        "\\begin{bmatrix}\n",
        "\\text{b} & \\text{h} & \\text{n} & \\text{t} \\\\\n",
        "\\text{c} & \\text{i} & \\text{o} & \\text{u} \\\\\n",
        "\\text{d} & \\text{j} & \\text{p} & \\text{v} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The first variable contains the letters input to the network, while the second\n",
        "contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "Note that despite the name of the function, we are cutting the data in the\n",
        "temporal dimension, since we already divided data into batches in the previous\n",
        "function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V36s8RYg37y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(source, i, seq_len, evaluation=False):\n",
        "    # Deal with the possibility that there's not enough data left for a full sequence\n",
        "    data = source[i:i+seq_len]\n",
        "    # Take the input data - shift by one for the target data\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZcHX50C37y6",
        "colab_type": "code",
        "outputId": "86a6411d-965c-44e0-92a4-7fa6df0229a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Examples and visualization\n",
        "batch_size = 100\n",
        "eval_batch_size = 4\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2754, 100])\n",
            "torch.Size([11986, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epM_cW_U37zA",
        "colab_type": "code",
        "outputId": "2be05230-e738-4f7d-ee07-79f79300fad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#Examples and visualization\n",
        "input_words, target_words = get_batch(val_data, 0, 3)\n",
        "pp.pprint(input_words)\n",
        "pp.pprint(target_words)\n",
        "input_words, target_words = get_batch(val_data, 3, 3)\n",
        "pp.pprint(input_words)\n",
        "pp.pprint(target_words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    0,    10,    15,    91],\n",
            "        [    1,  3018,   735,    13],\n",
            "        [17642,   187,   766,   496]])\n",
            "tensor([    1,  3018,   735,    13, 17642,   187,   766,   496, 17643,   827,\n",
            "          751,   131])\n",
            "tensor([[17643,   827,   751,   131],\n",
            "        [    1,    19,  4659,  2200],\n",
            "        [    0,    17,  2466,    22]])\n",
            "tensor([   1,   19, 4659, 2200,    0,   17, 2466,   22,    0, 3069,   39, 5521])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU_xRXAg37zF",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Cells in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oa43oaF37zG",
        "colab_type": "text"
      },
      "source": [
        "LSTMs expect inputs having 3 dimensions:\n",
        "- The first dimension is the temporal dimension, along which we (in our case) have the different words\n",
        "- The second dimension is the batch dimension, along which we stack the independant batches\n",
        "- The third dimension is the feature dimension, along which are the features of the vector representing the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp1F3E3s37zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a toy example of LSTM: \n",
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVbBQYfr37zS",
        "colab_type": "text"
      },
      "source": [
        "In our toy case, we have inputs and outputs containing 3 features (third dimension !)\n",
        "We created a sequence of 5 different inputs (first dimension !)\n",
        "We don't use batch (the second dimension will have one lement)\n",
        "\n",
        "\n",
        "We need an initial hidden state, of the right sizes for dimension 2/3, but with only one temporal element:\n",
        "Here, it is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4E_-qz037zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HByq4HuW37zc",
        "colab_type": "text"
      },
      "source": [
        "Why do we create a tuple of two tensors ? Because we use LSTMs: remember that they use two sets of weights,\n",
        "and two hidden states (Hidden state, and Cell state).\n",
        "If you don't remember, read https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "If we used a classic RNN, we would simply have ```hidden = torch.randn(1, 1, 3)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlno02B037zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The naive way of applying a lstm to inputs is to apply it one step at a time, and loop through the sequence\n",
        "for i in inputs:\n",
        "    # After each step, hidden contains the hidden states (remember, it's a tuple of two states).\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUe_9-t37zj",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, we can do the entire sequence all at once.\n",
        "The first value returned by LSTM is all of the Hidden states throughout the sequence.\n",
        "The second is just the most recent Hidden state and Cell state (you can compare the values)\n",
        "The reason for this is that:\n",
        "- ```out``` will give you access to all hidden states in the sequence, for each temporal step\n",
        "- ```hidden``` will allow you to continue the sequence and backpropagate later, with another sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaWXbkNP37zk",
        "colab_type": "code",
        "outputId": "accebc76-4b96-4df0-c1e6-4bd544ad35f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # Re-initialize\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "pp.pprint(out)\n",
        "pp.pprint(hidden)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0387,  0.1632, -0.2288]],\n",
            "\n",
            "        [[ 0.1990, -0.0031, -0.0425]],\n",
            "\n",
            "        [[ 0.1769, -0.0440,  0.0021]],\n",
            "\n",
            "        [[ 0.1846, -0.1151,  0.0203]],\n",
            "\n",
            "        [[ 0.1893, -0.2541,  0.0488]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.1893, -0.2541,  0.0488]]], grad_fn=<StackBackward>),\n",
            " tensor([[[ 0.4443, -0.5814,  0.0779]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYGlZoM737zr",
        "colab_type": "text"
      },
      "source": [
        "### Creating our own LSTM Model\n",
        "\n",
        "In Pytorch, models are usually implemented as custom ```nn.Module``` subclass:\n",
        "- We need to redefine the ```__init__``` method, which creates the object\n",
        "- We also need to redefine the ```forward``` method, which transform the input into outputs\n",
        "- We can also add any method that we need: here, in order to initiate weights in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670Y0G_b37zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "        # Initialize non-reccurent weights \n",
        "        self.init_weights()\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.ntoken = ntoken\n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
        "        weight = next(self.parameters())\n",
        "        hidden = (weight.new_zeros(self.nlayers, batch_size, self.nhid),weight.new_zeros(self.nlayers, batch_size, self.nhid))\n",
        "        return hidden  \n",
        "\n",
        "    def forward(self, input, hidden, return_h=False):\n",
        "        # Process the input\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        # Apply the LSTMs\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # Decode into scores\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        decoded =torch.nn.functional.log_softmax(decoded, dim=1)\n",
        "        return decoded, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLS-voaW37zx",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyYyfgLQ37z0",
        "colab_type": "code",
        "outputId": "a4537957-b2e8-45b9-dbaf-124902f6ffef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# If you have Cuda installed and a GPU available\n",
        "cuda = False\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n",
        "        \n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: You have a CUDA device, so you should probably choose cuda = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsavgZHx37z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 200\n",
        "hidden_size = 200\n",
        "layers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "vocab_size =len(corpus.dictionary.word2idx) # A compléter\n",
        "model = LSTMModel(vocab_size,embedding_size, hidden_size, layers)# A compléter - build the model from the class !\n",
        "model = model.to(device)\n",
        "params = list(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUesz78370A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 10.0\n",
        "optimizer = 'sgd'\n",
        "wdecay = 1.2e-6\n",
        "# For gradient clipping\n",
        "clip = 0.25\n",
        "\n",
        "# Create the optimizer\n",
        "optim =  torch.optim.SGD(params, lr=lr,weight_decay=wdecay)\n",
        "\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcuZXgb_370K",
        "colab_type": "text"
      },
      "source": [
        "Let's think about gradient propagation:\n",
        "\n",
        "We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
        "the next call to LSTM. This way, we can back-propagate the gradient for as long as we want.\n",
        "However, this puts a huge strain on the memory used by the model, since it implies retaining\n",
        "a always-growing number of tensors of gradients in the cache.\n",
        "\n",
        "\n",
        "We decide to not backpropagate through time beyond the current sequence ! \n",
        "We use a specific function to **cut the 'hidden/state cell' states from their previous dependencies**\n",
        "before using them to initialize the next call to the LSTM.\n",
        "This is done with the ```.detach()``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlk_fGkv370M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return  h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVBBSXf370R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Other global parameters\n",
        "epochs = 10\n",
        "seq_len = 30\n",
        "log_interval = 10\n",
        "save = 'model.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaBMtEEk370Y",
        "colab_type": "text"
      },
      "source": [
        "We now have everything necessary to define the training loop. \n",
        "Note that ```nn.Module``` objects override the ```__call__``` operator so you can call them like functions, which will have the same effect as calling their ```.forward()``` method:\n",
        "\n",
        "\n",
        "In practice, we use ```outputs = model(inputs)``` rather than ``` outputs = model.forward(inputs)```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1oCepRX370a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    # Initialize the hidden/cell state\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        # Get the input/target data\n",
        "        data, targets = get_batch(train_data, i,seq_len)\n",
        "        if  data.size()[0]*data.size()[1] ==targets.size()[0]:\n",
        "          # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "          # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "         # optim.zero_grad()\n",
        "          model.zero_grad()\n",
        "        \n",
        "\n",
        "          # Do the training loop: careful, look into the documentation for the criterion CrossEntropyLoss()\n",
        "          hidden = repackage_hidden(hidden)\n",
        "          output, hidden = model(data, hidden)\n",
        "          # Do the gradient clipping with the function torch.nn.utils.clip_grad_norm_, then the optimization step\n",
        "          \n",
        "          # We use .data to only accumulate the loss, and not keep track of the gradient too\n",
        "          loss = criterion(output, targets)\n",
        "          loss.backward()\n",
        "          \n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "         # optim.step()     \n",
        "          total_loss += loss.item()\n",
        "          \n",
        "          if batch % log_interval == 0 and batch > 0:\n",
        "              cur_loss = total_loss / log_interval\n",
        "              elapsed = time.time() - start_time\n",
        "              print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                      'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                  epoch, batch, len(train_data) // seq_len, lr,\n",
        "                  elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "              total_loss = 0\n",
        "              start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj_eGdBD370g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    # Initialize the hidden/cell state\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
        "            data, targets = get_batch(data_source, i,seq_len)\n",
        "            if  data.size()[0]*data.size()[1] ==targets.size()[0]:               \n",
        "              output, hidden = model(data, hidden)\n",
        "              hidden = repackage_hidden(hidden)\n",
        "              total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMgkCkSC370q",
        "colab_type": "code",
        "outputId": "05ee58e6-4dd4-4a4c-8db5-2fc876207884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # After loading, the parameters are not a continuous chunk of memory\n",
        "    # This makes them a continuous chunk, and will speed up the forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    10/   91 batches | lr 10.00 | ms/batch 1851.09 | loss  9.81 | ppl 18189.03\n",
            "| epoch   1 |    20/   91 batches | lr 10.00 | ms/batch 1621.92 | loss  8.11 | ppl  3323.08\n",
            "| epoch   1 |    30/   91 batches | lr 10.00 | ms/batch 1623.63 | loss  7.76 | ppl  2351.20\n",
            "| epoch   1 |    40/   91 batches | lr 10.00 | ms/batch 1624.37 | loss  7.60 | ppl  1994.93\n",
            "| epoch   1 |    50/   91 batches | lr 10.00 | ms/batch 1622.77 | loss  7.48 | ppl  1778.66\n",
            "| epoch   1 |    60/   91 batches | lr 10.00 | ms/batch 1642.62 | loss  7.47 | ppl  1763.24\n",
            "| epoch   1 |    70/   91 batches | lr 10.00 | ms/batch 1616.57 | loss  7.38 | ppl  1610.15\n",
            "| epoch   1 |    80/   91 batches | lr 10.00 | ms/batch 1618.10 | loss  7.41 | ppl  1657.04\n",
            "| epoch   1 |    90/   91 batches | lr 10.00 | ms/batch 1610.99 | loss  7.35 | ppl  1551.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 157.88s | valid loss  7.25 | valid ppl  1411.26\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |    10/   91 batches | lr 10.00 | ms/batch 1802.00 | loss  7.99 | ppl  2941.74\n",
            "| epoch   2 |    20/   91 batches | lr 10.00 | ms/batch 1627.18 | loss  7.31 | ppl  1499.34\n",
            "| epoch   2 |    30/   91 batches | lr 10.00 | ms/batch 1640.13 | loss  7.26 | ppl  1418.29\n",
            "| epoch   2 |    40/   91 batches | lr 10.00 | ms/batch 1609.35 | loss  7.23 | ppl  1380.82\n",
            "| epoch   2 |    50/   91 batches | lr 10.00 | ms/batch 1623.48 | loss  7.16 | ppl  1286.17\n",
            "| epoch   2 |    60/   91 batches | lr 10.00 | ms/batch 1634.09 | loss  7.15 | ppl  1274.51\n",
            "| epoch   2 |    70/   91 batches | lr 10.00 | ms/batch 1646.91 | loss  7.11 | ppl  1225.28\n",
            "| epoch   2 |    80/   91 batches | lr 10.00 | ms/batch 1612.82 | loss  7.09 | ppl  1205.33\n",
            "| epoch   2 |    90/   91 batches | lr 10.00 | ms/batch 1613.22 | loss  7.08 | ppl  1190.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 157.66s | valid loss  6.93 | valid ppl  1022.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    10/   91 batches | lr 10.00 | ms/batch 1788.90 | loss  7.72 | ppl  2244.53\n",
            "| epoch   3 |    20/   91 batches | lr 10.00 | ms/batch 1619.88 | loss  7.01 | ppl  1110.81\n",
            "| epoch   3 |    30/   91 batches | lr 10.00 | ms/batch 1614.91 | loss  6.99 | ppl  1086.75\n",
            "| epoch   3 |    40/   91 batches | lr 10.00 | ms/batch 1613.97 | loss  6.99 | ppl  1087.67\n",
            "| epoch   3 |    50/   91 batches | lr 10.00 | ms/batch 1616.59 | loss  6.92 | ppl  1014.28\n",
            "| epoch   3 |    60/   91 batches | lr 10.00 | ms/batch 1628.36 | loss  6.91 | ppl  1004.85\n",
            "| epoch   3 |    70/   91 batches | lr 10.00 | ms/batch 1627.91 | loss  6.90 | ppl   989.24\n",
            "| epoch   3 |    80/   91 batches | lr 10.00 | ms/batch 1611.98 | loss  6.88 | ppl   969.56\n",
            "| epoch   3 |    90/   91 batches | lr 10.00 | ms/batch 1620.47 | loss  6.87 | ppl   966.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 156.96s | valid loss  6.69 | valid ppl   802.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    10/   91 batches | lr 10.00 | ms/batch 1802.14 | loss  7.48 | ppl  1764.35\n",
            "| epoch   4 |    20/   91 batches | lr 10.00 | ms/batch 1660.63 | loss  6.81 | ppl   904.10\n",
            "| epoch   4 |    30/   91 batches | lr 10.00 | ms/batch 1668.51 | loss  6.76 | ppl   865.55\n",
            "| epoch   4 |    40/   91 batches | lr 10.00 | ms/batch 1641.26 | loss  6.76 | ppl   864.10\n",
            "| epoch   4 |    50/   91 batches | lr 10.00 | ms/batch 1647.52 | loss  6.75 | ppl   850.42\n",
            "| epoch   4 |    60/   91 batches | lr 10.00 | ms/batch 1651.26 | loss  6.74 | ppl   848.29\n",
            "| epoch   4 |    70/   91 batches | lr 10.00 | ms/batch 1651.94 | loss  6.69 | ppl   805.49\n",
            "| epoch   4 |    80/   91 batches | lr 10.00 | ms/batch 1631.26 | loss  6.69 | ppl   804.55\n",
            "| epoch   4 |    90/   91 batches | lr 10.00 | ms/batch 1634.46 | loss  6.71 | ppl   821.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 159.58s | valid loss  6.53 | valid ppl   686.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    10/   91 batches | lr 10.00 | ms/batch 1804.82 | loss  7.29 | ppl  1464.86\n",
            "| epoch   5 |    20/   91 batches | lr 10.00 | ms/batch 1646.12 | loss  6.65 | ppl   769.66\n",
            "| epoch   5 |    30/   91 batches | lr 10.00 | ms/batch 1626.04 | loss  6.60 | ppl   736.69\n",
            "| epoch   5 |    40/   91 batches | lr 10.00 | ms/batch 1623.37 | loss  6.60 | ppl   735.56\n",
            "| epoch   5 |    50/   91 batches | lr 10.00 | ms/batch 1627.61 | loss  6.57 | ppl   716.17\n",
            "| epoch   5 |    60/   91 batches | lr 10.00 | ms/batch 1626.65 | loss  6.58 | ppl   718.21\n",
            "| epoch   5 |    70/   91 batches | lr 10.00 | ms/batch 1639.05 | loss  6.55 | ppl   701.88\n",
            "| epoch   5 |    80/   91 batches | lr 10.00 | ms/batch 1626.27 | loss  6.56 | ppl   708.19\n",
            "| epoch   5 |    90/   91 batches | lr 10.00 | ms/batch 1627.90 | loss  6.56 | ppl   706.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 158.18s | valid loss  6.43 | valid ppl   617.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    10/   91 batches | lr 10.00 | ms/batch 1790.79 | loss  7.17 | ppl  1296.88\n",
            "| epoch   6 |    20/   91 batches | lr 10.00 | ms/batch 1651.94 | loss  6.52 | ppl   681.87\n",
            "| epoch   6 |    30/   91 batches | lr 10.00 | ms/batch 1615.85 | loss  6.45 | ppl   631.89\n",
            "| epoch   6 |    40/   91 batches | lr 10.00 | ms/batch 1619.75 | loss  6.51 | ppl   668.58\n",
            "| epoch   6 |    50/   91 batches | lr 10.00 | ms/batch 1623.54 | loss  6.46 | ppl   638.88\n",
            "| epoch   6 |    60/   91 batches | lr 10.00 | ms/batch 1637.27 | loss  6.48 | ppl   650.85\n",
            "| epoch   6 |    70/   91 batches | lr 10.00 | ms/batch 1636.38 | loss  6.41 | ppl   610.89\n",
            "| epoch   6 |    80/   91 batches | lr 10.00 | ms/batch 1619.95 | loss  6.42 | ppl   611.60\n",
            "| epoch   6 |    90/   91 batches | lr 10.00 | ms/batch 1623.52 | loss  6.46 | ppl   638.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 157.80s | valid loss  6.39 | valid ppl   594.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    10/   91 batches | lr 10.00 | ms/batch 1786.03 | loss  7.05 | ppl  1153.49\n",
            "| epoch   7 |    20/   91 batches | lr 10.00 | ms/batch 1635.60 | loss  6.43 | ppl   618.91\n",
            "| epoch   7 |    30/   91 batches | lr 10.00 | ms/batch 1619.57 | loss  6.37 | ppl   586.56\n",
            "| epoch   7 |    40/   91 batches | lr 10.00 | ms/batch 1612.63 | loss  6.40 | ppl   601.14\n",
            "| epoch   7 |    50/   91 batches | lr 10.00 | ms/batch 1616.51 | loss  6.34 | ppl   565.18\n",
            "| epoch   7 |    60/   91 batches | lr 10.00 | ms/batch 1612.62 | loss  6.39 | ppl   593.58\n",
            "| epoch   7 |    70/   91 batches | lr 10.00 | ms/batch 1628.33 | loss  6.32 | ppl   556.75\n",
            "| epoch   7 |    80/   91 batches | lr 10.00 | ms/batch 1610.19 | loss  6.34 | ppl   564.03\n",
            "| epoch   7 |    90/   91 batches | lr 10.00 | ms/batch 1611.75 | loss  6.36 | ppl   581.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 156.89s | valid loss  6.21 | valid ppl   498.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    10/   91 batches | lr 10.00 | ms/batch 1780.66 | loss  6.92 | ppl  1012.36\n",
            "| epoch   8 |    20/   91 batches | lr 10.00 | ms/batch 1642.12 | loss  6.32 | ppl   554.06\n",
            "| epoch   8 |    30/   91 batches | lr 10.00 | ms/batch 1627.89 | loss  6.29 | ppl   538.55\n",
            "| epoch   8 |    40/   91 batches | lr 10.00 | ms/batch 1612.20 | loss  6.31 | ppl   551.87\n",
            "| epoch   8 |    50/   91 batches | lr 10.00 | ms/batch 1631.02 | loss  6.27 | ppl   527.75\n",
            "| epoch   8 |    60/   91 batches | lr 10.00 | ms/batch 1607.56 | loss  6.28 | ppl   531.93\n",
            "| epoch   8 |    70/   91 batches | lr 10.00 | ms/batch 1628.08 | loss  6.23 | ppl   510.20\n",
            "| epoch   8 |    80/   91 batches | lr 10.00 | ms/batch 1615.80 | loss  6.26 | ppl   523.28\n",
            "| epoch   8 |    90/   91 batches | lr 10.00 | ms/batch 1613.20 | loss  6.27 | ppl   527.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 157.18s | valid loss  6.23 | valid ppl   508.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    10/   91 batches | lr 2.50 | ms/batch 1776.01 | loss  6.76 | ppl   864.20\n",
            "| epoch   9 |    20/   91 batches | lr 2.50 | ms/batch 1618.72 | loss  6.14 | ppl   464.09\n",
            "| epoch   9 |    30/   91 batches | lr 2.50 | ms/batch 1634.02 | loss  6.11 | ppl   450.54\n",
            "| epoch   9 |    40/   91 batches | lr 2.50 | ms/batch 1606.04 | loss  6.13 | ppl   458.76\n",
            "| epoch   9 |    50/   91 batches | lr 2.50 | ms/batch 1616.83 | loss  6.08 | ppl   439.15\n",
            "| epoch   9 |    60/   91 batches | lr 2.50 | ms/batch 1606.54 | loss  6.11 | ppl   451.94\n",
            "| epoch   9 |    70/   91 batches | lr 2.50 | ms/batch 1613.54 | loss  6.07 | ppl   434.67\n",
            "| epoch   9 |    80/   91 batches | lr 2.50 | ms/batch 1627.27 | loss  6.09 | ppl   440.97\n",
            "| epoch   9 |    90/   91 batches | lr 2.50 | ms/batch 1610.13 | loss  6.12 | ppl   454.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 156.57s | valid loss  6.06 | valid ppl   426.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    10/   91 batches | lr 2.50 | ms/batch 1785.08 | loss  6.70 | ppl   814.58\n",
            "| epoch  10 |    20/   91 batches | lr 2.50 | ms/batch 1621.73 | loss  6.10 | ppl   447.62\n",
            "| epoch  10 |    30/   91 batches | lr 2.50 | ms/batch 1632.70 | loss  6.07 | ppl   433.05\n",
            "| epoch  10 |    40/   91 batches | lr 2.50 | ms/batch 1604.38 | loss  6.10 | ppl   445.81\n",
            "| epoch  10 |    50/   91 batches | lr 2.50 | ms/batch 1619.92 | loss  6.05 | ppl   424.94\n",
            "| epoch  10 |    60/   91 batches | lr 2.50 | ms/batch 1605.10 | loss  6.09 | ppl   441.13\n",
            "| epoch  10 |    70/   91 batches | lr 2.50 | ms/batch 1606.55 | loss  6.05 | ppl   422.84\n",
            "| epoch  10 |    80/   91 batches | lr 2.50 | ms/batch 1628.31 | loss  6.07 | ppl   430.85\n",
            "| epoch  10 |    90/   91 batches | lr 2.50 | ms/batch 1609.87 | loss  6.09 | ppl   440.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 156.61s | valid loss  6.04 | valid ppl   419.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.09 | test ppl   440.68\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAEFGez8370y",
        "colab_type": "text"
      },
      "source": [
        "To play with your LSTM Language model, implement a function to generate a fixed number of words given an input text. You will need to use every part of the pipeline to: \n",
        "- Process the input text into words and then a series of word indexes using the vocabulary\n",
        "- Do inference on this input using the model\n",
        "- Loop in order to generate the next word **greedily** as many times as needed\n",
        "You can choose the next word by doing multinomial sampling from the output distribution (which you can control using softmax temperature) or simply using the argmax. Good Luck ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIHmK30Q7KwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwlpQNiN65mH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8e3fb5ec-8906-445a-e4cd-d0eafba380d8"
      },
      "source": [
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (encoder): Embedding(19482, 200)\n",
              "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.5)\n",
              "  (decoder): Linear(in_features=200, out_features=19482, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuUjQ3de5vTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = 'wikitext-2-small/'\n",
        "corpus = Corpus(data)\n",
        "ntokens = len(corpus.dictionary)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dFLV4zX7jUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden = model.init_hidden(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCB_JEEb7mrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efMfqv1o7pwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "a143e1ce-15cb-4acf-e8f2-b6943c20e661"
      },
      "source": [
        "with open(\"out.txt\", 'w') as outf:\n",
        "    with torch.no_grad(): \n",
        "        for i in range(100):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(1.0).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, 100))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/100 words\n",
            "| Generated 10/100 words\n",
            "| Generated 20/100 words\n",
            "| Generated 30/100 words\n",
            "| Generated 40/100 words\n",
            "| Generated 50/100 words\n",
            "| Generated 60/100 words\n",
            "| Generated 70/100 words\n",
            "| Generated 80/100 words\n",
            "| Generated 90/100 words\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}